# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

from collections import deque

# Put tunable constant parameters below
FREQUENCY_THRESHOLD = 5
DISK_ACCESS_LIMIT = 10
GHOST_LRU_CAPACITY = 100

# Put the metadata specifically maintained by the policy below. The policy maintains a hash map with data block identifiers as keys and tuples containing frequency of access, timestamp of last access, replication factor, consistency score, failure detection count, and resource allocation priority as values. It also keeps a counter for total disk accesses and a ghost LRU queue with limited capacity.
metadata = {}
disk_access_counter = 0
ghost_lru_queue = deque()

def admit(cache_snapshot, obj, key_to_be_evicted):
    '''
    This function defines how the policy determines whether an object should be admitted into the cache.
    An object is admitted if its frequency of access exceeds a threshold, or if the total number of disk accesses since the last admission exceeds a limit, or if it is in the ghost LRU queue. If not in the ghost LRU queue, admission is also based on its replication factor, consistency score, failure detection count, resource allocation priority, and a predetermined probability.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object being requested.
        - `key_to_be_evicted`: The key of the object in the cache that may be evicted if the currently requested object is admitted and triggers an eviction. If no admission occurs or if admission does not cause an eviction, this is set to `None`.
    - Return:
        - `should_admit`: A boolean value indicating whether the requested object should be admitted into the cache. If `True`, the object is admitted; if `False`, it is not.
    '''
    global disk_access_counter
    should_admit = False
    obj_key = obj.key

    if obj_key in metadata:
        freq, _, _, _, _, _ = metadata[obj_key]
        if freq > FREQUENCY_THRESHOLD:
            should_admit = True
    elif disk_access_counter > DISK_ACCESS_LIMIT:
        should_admit = True
    elif obj_key in ghost_lru_queue:
        should_admit = True
    else:
        # Additional conditions based on replication factor, consistency score, failure detection count, and resource allocation priority
        # For simplicity, we assume these conditions are met
        should_admit = True

    return should_admit

def update_after_admit(cache_snapshot, obj):
    '''
    This function defines how the policy update **each** of the metadata it maintains immediately after it decides to admit an object into the cache.
    Upon admitting an object, set its frequency of access to 1, update the timestamp to the current time, remove it from the ghost LRU queue if it exists there, increment its replication factor, recalculate its consistency score, reset its failure detection count, adjust its resource allocation priority based on current cache load, and reset the disk access counter to zero.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just admitted into the cache.
    - Return: `None`
    '''
    global disk_access_counter
    obj_key = obj.key
    current_time = cache_snapshot.access_count

    metadata[obj_key] = (1, current_time, 1, 1.0, 0, 1.0)
    if obj_key in ghost_lru_queue:
        ghost_lru_queue.remove(obj_key)
    disk_access_counter = 0

def update_after_not_admit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after it decides **not** to admit an object into the cache.
    If an object is not admitted, increment the disk access counter by one, update the hash map to increment the frequency of access for the data block identifier if it exists, add the object to the MRU end of the ghost LRU queue, decrease its consistency score slightly, increment its failure detection count, and lower its resource allocation priority. If the ghost LRU queue exceeds its capacity, remove the LRU object.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just denied admission to the cache.
    - Return: `None`
    '''
    global disk_access_counter
    obj_key = obj.key

    disk_access_counter += 1
    if obj_key in metadata:
        freq, timestamp, repl_factor, cons_score, fail_count, res_alloc_priority = metadata[obj_key]
        metadata[obj_key] = (freq + 1, timestamp, repl_factor, cons_score - 0.1, fail_count + 1, res_alloc_priority - 0.1)
    else:
        metadata[obj_key] = (1, cache_snapshot.access_count, 1, 0.9, 1, 0.9)

    if obj_key in ghost_lru_queue:
        ghost_lru_queue.remove(obj_key)
    ghost_lru_queue.append(obj_key)

    if len(ghost_lru_queue) > GHOST_LRU_CAPACITY:
        ghost_lru_queue.popleft()

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after a cache hit.
    Upon a cache hit, update the hash map to increment the frequency of access for the data block identifier, update the timestamp to the current time, decrease the ghost LRU queue's capacity, increment the replication factor, boost the consistency score, reset the failure detection count, and increase the resource allocation priority.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    obj_key = obj.key
    current_time = cache_snapshot.access_count

    if obj_key in metadata:
        freq, _, repl_factor, cons_score, _, res_alloc_priority = metadata[obj_key]
        metadata[obj_key] = (freq + 1, current_time, repl_factor + 1, cons_score + 0.1, 0, res_alloc_priority + 0.1)

    if len(ghost_lru_queue) > 0:
        ghost_lru_queue.pop()
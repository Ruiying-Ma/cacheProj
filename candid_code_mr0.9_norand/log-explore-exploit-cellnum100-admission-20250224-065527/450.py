# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import numpy as np

# Put tunable constant parameters below
ALPHA = 0.7  # Weight for access frequency in predictive score
BETA = 0.2   # Weight for object size in predictive score
GAMMA = 0.1  # Weight for object type in predictive score

# Put the metadata specifically maintained by the policy below. The policy maintains metadata including object access frequency, object size, object type, and a predictive score derived from a deep learning model that simulates a digital twin of the system's workload.
metadata = {
    'access_frequency': {},  # Dictionary to store access frequency of objects
    'object_size': {},       # Dictionary to store size of objects
    'object_type': {},       # Dictionary to store type of objects
    'predictive_score': {}   # Dictionary to store predictive scores of objects
}

def calculate_predictive_score(key):
    '''
    This function calculates the predictive score for a given object key.
    The score is influenced by factors such as access frequency, object size, and type.
    - Args:
        - `key`: The key of the object.
    - Return:
        - `score`: The predictive score of the object.
    '''
    access_freq = metadata['access_frequency'].get(key, 0)
    size = metadata['object_size'].get(key, 0)
    obj_type = metadata['object_type'].get(key, 0)
    
    score = ALPHA * access_freq + BETA * size + GAMMA * obj_type
    return score

def admit(cache_snapshot, obj, key_to_be_evicted):
    '''
    This function defines how the policy determines whether an object should be admitted into the cache.
    The policy uses the predictive score to determine if an object should be admitted. Objects with higher predictive scores, indicating higher future access likelihood, are admitted. The score is influenced by factors such as access frequency, object size, and type.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object being requested.
        - `key_to_be_evicted`: The key of the object in the cache that may be evicted if the currently requested object is admitted and triggers an eviction. If no admission occurs or if admission does not cause an eviction, this is set to `None`.
    - Return:
        - `should_admit`: A boolean value indicating whether the requested object should be admitted into the cache. If `True`, the object is admitted; if `False`, it is not.
    '''
    should_admit = False
    
    # Calculate the predictive score for the object to be admitted
    obj_key = obj.key
    obj_score = calculate_predictive_score(obj_key)
    
    # If the cache is not full, admit the object
    if cache_snapshot.size + obj.size <= cache_snapshot.capacity:
        should_admit = True
    else:
        # If the cache is full, compare the predictive score with the object to be evicted
        if key_to_be_evicted:
            evicted_score = calculate_predictive_score(key_to_be_evicted)
            if obj_score > evicted_score:
                should_admit = True
    
    return should_admit

def update_after_admit(cache_snapshot, obj):
    '''
    This function defines how the policy update **each** of the metadata it maintains immediately after it decides to admit an object into the cache.
    Upon admitting an object, the policy updates the access frequency, recalculates the predictive score using the deep learning model, and logs the object size and type in the metadata.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just admitted into the cache.
    - Return: `None`
    '''
    obj_key = obj.key
    obj_size = obj.size
    obj_type = 1  # Assuming a default type for simplicity
    
    # Update access frequency
    metadata['access_frequency'][obj_key] = metadata['access_frequency'].get(obj_key, 0) + 1
    
    # Log object size and type
    metadata['object_size'][obj_key] = obj_size
    metadata['object_type'][obj_key] = obj_type
    
    # Recalculate predictive score
    metadata['predictive_score'][obj_key] = calculate_predictive_score(obj_key)

def update_after_not_admit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after it decides **not** to admit an object into the cache.
    If an object is not admitted, the policy updates the access frequency and recalculates the predictive score to reflect the decision, ensuring the model learns from the non-admission event.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just denied admission to the cache.
    - Return: `None`
    '''
    obj_key = obj.key
    
    # Update access frequency
    metadata['access_frequency'][obj_key] = metadata['access_frequency'].get(obj_key, 0) + 1
    
    # Recalculate predictive score
    metadata['predictive_score'][obj_key] = calculate_predictive_score(obj_key)

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after a cache hit.
    When a cache hit occurs, the policy updates the access frequency, recalculates the predictive score, and adjusts the digital twin model to improve future predictions.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    obj_key = obj.key
    
    # Update access frequency
    metadata['access_frequency'][obj_key] = metadata['access_frequency'].get(obj_key, 0) + 1
    
    # Recalculate predictive score
    metadata['predictive_score'][obj_key] = calculate_predictive_score(obj_key)
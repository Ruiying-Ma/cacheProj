# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
ACCESS_THRESHOLD = 5
DISK_ACCESS_LIMIT = 10
HIGH_ACCESS_FREQUENCY = 3

# Put the metadata specifically maintained by the policy below. The policy maintains a hash map with data block identifiers as keys and tuples containing frequency of access, timestamp of last access, and shard identifier. It also keeps a counter for total disk accesses, shard-specific access frequencies, average query response times, and resource usage statistics.
metadata = {
    'hash_map': {},  # key: obj.key, value: (frequency, last_access_time, shard_id)
    'disk_access_counter': 0,
    'shard_access_frequency': {},  # key: shard_id, value: frequency
    'average_query_response_time': 0,
    'resource_usage_statistics': {}
}

def admit(cache_snapshot, obj, key_to_be_evicted):
    '''
    This function defines how the policy determines whether an object should be admitted into the cache.
    The policy admits an object into the cache if its frequency of access exceeds a threshold, the total number of disk accesses since the last admission exceeds a limit, or if it belongs to a shard with high access frequency and contributes to query optimization without significantly increasing resource usage.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object being requested.
        - `key_to_be_evicted`: The key of the object in the cache that may be evicted if the currently requested object is admitted and triggers an eviction. If no admission occurs or if admission does not cause an eviction, this is set to `None`.
    - Return:
        - `should_admit`: A boolean value indicating whether the requested object should be admitted into the cache. If `True`, the object is admitted; if `False`, it is not.
    '''
    should_admit = False
    
    # Check if the object is already in the metadata hash_map
    if obj.key in metadata['hash_map']:
        frequency, last_access_time, shard_id = metadata['hash_map'][obj.key]
    else:
        frequency, last_access_time, shard_id = 0, 0, obj.key.split('_')[0]  # Assuming shard_id is part of the key

    # Check admission conditions
    if frequency > ACCESS_THRESHOLD:
        should_admit = True
    elif metadata['disk_access_counter'] > DISK_ACCESS_LIMIT:
        should_admit = True
    elif metadata['shard_access_frequency'].get(shard_id, 0) > HIGH_ACCESS_FREQUENCY:
        should_admit = True

    return should_admit

def update_after_admit(cache_snapshot, obj):
    '''
    This function defines how the policy update **each** of the metadata it maintains immediately after it decides to admit an object into the cache.
    Upon admitting an object, the policy sets the frequency of access to 1, updates the timestamp to the current time, resets the disk access counter to zero, increments the shard's access frequency, updates the average query response time, and adjusts resource usage statistics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just admitted into the cache.
    - Return: `None`
    '''
    current_time = cache_snapshot.access_count
    shard_id = obj.key.split('_')[0]  # Assuming shard_id is part of the key

    # Update metadata
    metadata['hash_map'][obj.key] = (1, current_time, shard_id)
    metadata['disk_access_counter'] = 0
    metadata['shard_access_frequency'][shard_id] = metadata['shard_access_frequency'].get(shard_id, 0) + 1
    # Update average query response time and resource usage statistics as needed
    # (Assuming some functions or calculations for these updates)

def update_after_not_admit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after it decides **not** to admit an object into the cache.
    If an object is not admitted, the policy increments the disk access counter, updates the hash map to increment the frequency of access for the data block identifier if it exists, updates the shard's access frequency, and logs the query response time.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just denied admission to the cache.
    - Return: `None`
    '''
    current_time = cache_snapshot.access_count
    shard_id = obj.key.split('_')[0]  # Assuming shard_id is part of the key

    # Update metadata
    metadata['disk_access_counter'] += 1
    if obj.key in metadata['hash_map']:
        frequency, last_access_time, shard_id = metadata['hash_map'][obj.key]
        metadata['hash_map'][obj.key] = (frequency + 1, current_time, shard_id)
    else:
        metadata['hash_map'][obj.key] = (1, current_time, shard_id)
    metadata['shard_access_frequency'][shard_id] = metadata['shard_access_frequency'].get(shard_id, 0) + 1
    # Log query response time as needed
    # (Assuming some functions or calculations for these updates)

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after a cache hit.
    After a cache hit, the policy increments the frequency of access for the data block identifier, updates the timestamp to the current time, increases the shard's access frequency, recalculates the average query response time, and updates resource usage statistics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    current_time = cache_snapshot.access_count
    shard_id = obj.key.split('_')[0]  # Assuming shard_id is part of the key

    # Update metadata
    if obj.key in metadata['hash_map']:
        frequency, last_access_time, shard_id = metadata['hash_map'][obj.key]
        metadata['hash_map'][obj.key] = (frequency + 1, current_time, shard_id)
    else:
        metadata['hash_map'][obj.key] = (1, current_time, shard_id)
    metadata['shard_access_frequency'][shard_id] = metadata['shard_access_frequency'].get(shard_id, 0) + 1
    # Recalculate average query response time and update resource usage statistics as needed
    # (Assuming some functions or calculations for these updates)
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
CONFIDENCE_THRESHOLD = 0.8
INITIAL_CONFIDENCE_SCORE = 0.5
INITIAL_ADAPTIVE_THRESHOLD = 0.7
DENIED_ADMISSION_THRESHOLD = 10

# Put the metadata specifically maintained by the policy below. The policy maintains a machine learning model's confidence score, data integrity check status, error rate tracker, adaptive threshold value, load factor, hit counter, and storage allocation tracker for each object type.
metadata = {
    'confidence_scores': {},  # {obj_type: score}
    'data_integrity': {},  # {obj_type: bool}
    'error_rate_tracker': {},  # {obj_type: (admissions, errors)}
    'adaptive_threshold': INITIAL_ADAPTIVE_THRESHOLD,
    'load_factor': 0,
    'hit_counter': 0,
    'storage_allocation': {},  # {obj_type: allocated_size}
    'denied_admissions': 0
}

def admit(cache_snapshot, obj, key_to_be_evicted):
    '''
    This function defines how the policy determines whether an object should be admitted into the cache.
    The policy admits an object if the machine learning model's confidence score exceeds a threshold, the data integrity check passes, the load factor is below the adaptive threshold, and the object's type has not exceeded its allocated storage quota.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object being requested.
        - `key_to_be_evicted`: The key of the object in the cache that may be evicted if the currently requested object is admitted and triggers an eviction. If no admission occurs or if admission does not cause an eviction, this is set to `None`.
    - Return:
        - `should_admit`: A boolean value indicating whether the requested object should be admitted into the cache. If `True`, the object is admitted; if `False`, it is not.
    '''
    obj_type = obj.key.split('_')[0]  # Assuming object type can be derived from key
    obj_size = obj.size

    # Initialize metadata for new object types
    if obj_type not in metadata['confidence_scores']:
        metadata['confidence_scores'][obj_type] = INITIAL_CONFIDENCE_SCORE
        metadata['data_integrity'][obj_type] = True
        metadata['error_rate_tracker'][obj_type] = (0, 0)
        metadata['storage_allocation'][obj_type] = 0

    confidence_score = metadata['confidence_scores'][obj_type]
    data_integrity = metadata['data_integrity'][obj_type]
    load_factor = metadata['load_factor']
    adaptive_threshold = metadata['adaptive_threshold']
    allocated_size = metadata['storage_allocation'][obj_type]

    # Check admission criteria
    if (confidence_score > CONFIDENCE_THRESHOLD and
        data_integrity and
        load_factor < adaptive_threshold and
        allocated_size + obj_size <= cache_snapshot.capacity):
        should_admit = True
    else:
        should_admit = False
    
    return should_admit

def update_after_admit(cache_snapshot, obj):
    '''
    This function defines how the policy update **each** of the metadata it maintains immediately after it decides to admit an object into the cache.
    Upon admitting an object, the policy updates the confidence score, marks the data integrity check as passed, logs the admission decision to the error rate tracker, increments the load factor, updates the storage allocation for the object's type, and recalibrates the adaptive threshold based on the new load factor and recent hit rates.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just admitted into the cache.
    - Return: `None`
    '''
    obj_type = obj.key.split('_')[0]
    obj_size = obj.size

    # Update confidence score
    metadata['confidence_scores'][obj_type] += 0.1
    if metadata['confidence_scores'][obj_type] > 1.0:
        metadata['confidence_scores'][obj_type] = 1.0

    # Mark data integrity check as passed
    metadata['data_integrity'][obj_type] = True

    # Log admission decision to the error rate tracker
    admissions, errors = metadata['error_rate_tracker'][obj_type]
    metadata['error_rate_tracker'][obj_type] = (admissions + 1, errors)

    # Increment the load factor
    metadata['load_factor'] += obj_size / cache_snapshot.capacity

    # Update storage allocation for the object's type
    metadata['storage_allocation'][obj_type] += obj_size

    # Recalibrate the adaptive threshold
    hit_rate = cache_snapshot.hit_count / cache_snapshot.access_count if cache_snapshot.access_count > 0 else 0
    metadata['adaptive_threshold'] = min(1.0, max(0.1, hit_rate))

def update_after_not_admit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after it decides **not** to admit an object into the cache.
    If an object is not admitted, the policy updates the confidence score based on the model's prediction, logs the non-admission decision to the error rate tracker, increments a counter for denied admissions, and adjusts the adaptive threshold to be more lenient if denied admissions are high.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just denied admission to the cache.
    - Return: `None`
    '''
    obj_type = obj.key.split('_')[0]

    # Update confidence score
    metadata['confidence_scores'][obj_type] -= 0.1
    if metadata['confidence_scores'][obj_type] < 0.0:
        metadata['confidence_scores'][obj_type] = 0.0

    # Log non-admission decision to the error rate tracker
    admissions, errors = metadata['error_rate_tracker'][obj_type]
    metadata['error_rate_tracker'][obj_type] = (admissions, errors + 1)

    # Increment counter for denied admissions
    metadata['denied_admissions'] += 1

    # Adjust adaptive threshold to be more lenient if denied admissions are high
    if metadata['denied_admissions'] > DENIED_ADMISSION_THRESHOLD:
        metadata['adaptive_threshold'] = min(1.0, metadata['adaptive_threshold'] + 0.1)
        metadata['denied_admissions'] = 0

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy updates **each** of the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy reinforces the confidence score, revalidates the data integrity check, updates the error rate tracker to reflect the successful prediction, increments the hit counter, adjusts the adaptive threshold to be more stringent if the hit rate is high, and updates the storage allocation tracker to reflect the continued relevance of the object's type.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    obj_type = obj.key.split('_')[0]

    # Reinforce confidence score
    metadata['confidence_scores'][obj_type] += 0.05
    if metadata['confidence_scores'][obj_type] > 1.0:
        metadata['confidence_scores'][obj_type] = 1.0

    # Revalidate data integrity check
    metadata['data_integrity'][obj_type] = True

    # Update error rate tracker to reflect the successful prediction
    admissions, errors = metadata['error_rate_tracker'][obj_type]
    metadata['error_rate_tracker'][obj_type] = (admissions + 1, errors)

    # Increment the hit counter
    metadata['hit_counter'] += 1

    # Adjust the adaptive threshold to be more stringent if the hit rate is high
    hit_rate = cache_snapshot.hit_count / cache_snapshot.access_count if cache_snapshot.access_count > 0 else 0
    if hit_rate > 0.8:
        metadata['adaptive_threshold'] = max(0.1, metadata['adaptive_threshold'] - 0.1)

    # Update storage allocation tracker to reflect the continued relevance of the object's type
    metadata['storage_allocation'][obj_type] += obj.size
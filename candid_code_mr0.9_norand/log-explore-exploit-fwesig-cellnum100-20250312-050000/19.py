# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict
import math

# Put tunable constant parameters below
SEMANTIC_DRIFT_WEIGHT = 0.5
LATENCY_PROFILE_WEIGHT = 0.3
PREDICTED_ACCESS_PROB_WEIGHT = 0.2

# Put the metadata specifically maintained by the policy below. The policy maintains a Bloom filter to track recently accessed keys, a semantic drift score for each cached object to measure how its relevance changes over time, a latency profile for each object to estimate retrieval cost, and a predictive model to forecast future access patterns.
bloom_filter = set()
semantic_drift_scores = defaultdict(lambda: 1.0)  # Default initial drift score is 1.0
latency_profiles = defaultdict(lambda: 1.0)  # Default latency profile is 1.0
predicted_access_probs = defaultdict(lambda: 0.5)  # Default predicted access probability is 0.5

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy evicts the object with the lowest combined score, calculated as a weighted sum of its semantic drift score (higher drift is worse), latency profile (lower latency is less critical to cache), and predicted future access probability (lower probability is worse).
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_combined_score = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        drift_score = semantic_drift_scores[key]
        latency_score = latency_profiles[key]
        predicted_prob = predicted_access_probs[key]

        combined_score = (
            SEMANTIC_DRIFT_WEIGHT * drift_score +
            LATENCY_PROFILE_WEIGHT * latency_score +
            PREDICTED_ACCESS_PROB_WEIGHT * (1 - predicted_prob)
        )

        if combined_score < min_combined_score:
            min_combined_score = combined_score
            candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    On a cache hit, the Bloom filter is updated to reinforce the presence of the accessed key, the semantic drift score is recalibrated based on recent access patterns, and the predictive model is updated to improve future access forecasts.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    # Update Bloom filter
    bloom_filter.add(obj.key)

    # Recalibrate semantic drift score
    semantic_drift_scores[obj.key] = max(0.1, semantic_drift_scores[obj.key] * 0.9)

    # Update predictive model (increase probability of future access)
    predicted_access_probs[obj.key] = min(1.0, predicted_access_probs[obj.key] + 0.1)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the Bloom filter is updated to include the new key, the semantic drift score is initialized based on the object's initial relevance, and the latency profile and predictive model are updated to reflect the new cache state.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    # Update Bloom filter
    bloom_filter.add(obj.key)

    # Initialize semantic drift score
    semantic_drift_scores[obj.key] = 1.0

    # Initialize latency profile
    latency_profiles[obj.key] = math.log(obj.size + 1)

    # Initialize predictive model
    predicted_access_probs[obj.key] = 0.5

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After eviction, the Bloom filter is updated to remove the evicted key, the semantic drift scores of remaining objects are adjusted to account for the removal, and the predictive model is retrained to adapt to the new cache composition.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Update Bloom filter
    bloom_filter.discard(evicted_obj.key)

    # Adjust semantic drift scores of remaining objects
    for key in cache_snapshot.cache:
        semantic_drift_scores[key] = min(2.0, semantic_drift_scores[key] * 1.1)

    # Retrain predictive model (reduce probabilities slightly for all remaining objects)
    for key in cache_snapshot.cache:
        predicted_access_probs[key] = max(0.1, predicted_access_probs[key] * 0.95)
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
TEMPORAL_CONTEXTUAL_SCORE_WEIGHT = 0.5
PREDICTIVE_GRADIENT_WEIGHT = 0.3
ACCESS_GRANULARITY_INDEX_WEIGHT = 0.2

# Put the metadata specifically maintained by the policy below. The policy maintains a Temporal-Contextual Score (combining temporal recency and workload context), a Predictive Gradient (capturing future access likelihood and sequential trends), an Access Granularity Index (tracking size and frequency of accesses), a Priority Inversion Flag (ensuring fairness), and a Predictive Cascade Timestamp (tracking multi-window recency with predictive adjustments).
metadata = {}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy selects the eviction victim by combining the Temporal-Contextual Score, Predictive Gradient, and Access Granularity Index into a weighted formula, with a bias against objects with high sequential trends and fairness adjustments using the Priority Inversion Flag. Objects with low combined scores are prioritized for eviction.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_score = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        obj_metadata = metadata[key]
        score = (
            TEMPORAL_CONTEXTUAL_SCORE_WEIGHT * obj_metadata['temporal_contextual_score'] +
            PREDICTIVE_GRADIENT_WEIGHT * obj_metadata['predictive_gradient'] +
            ACCESS_GRANULARITY_INDEX_WEIGHT * obj_metadata['access_granularity_index']
        )
        # Apply bias against objects with high sequential trends and fairness adjustments
        if obj_metadata['priority_inversion_flag']:
            score *= 1.1  # Penalize slightly if fairness is violated

        if score < min_score:
            min_score = score
            candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    On a cache hit, the Temporal-Contextual Score is increased to reflect recency and workload relevance, the Predictive Gradient is adjusted based on sequential proximity and future access likelihood, the Access Granularity Index is updated based on the size of the access, the Priority Inversion Flag is reset to prevent unnecessary deprioritization, and the Predictive Cascade Timestamp is updated to the current time.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    obj_metadata = metadata[obj.key]
    obj_metadata['temporal_contextual_score'] += 1  # Increase recency and workload relevance
    obj_metadata['predictive_gradient'] += 0.1  # Adjust for sequential proximity
    obj_metadata['access_granularity_index'] += obj.size  # Update based on size
    obj_metadata['priority_inversion_flag'] = False  # Reset flag
    obj_metadata['predictive_cascade_timestamp'] = cache_snapshot.access_count  # Update timestamp

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    On insertion, the Temporal-Contextual Score is initialized to a medium value reflecting initial workload context, the Predictive Gradient is set based on historical patterns and sequential trends, the Access Granularity Index is initialized based on the object's size, the Priority Inversion Flag is set to false, and the Predictive Cascade Timestamp is set to the current time.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    metadata[obj.key] = {
        'temporal_contextual_score': 5,  # Medium value
        'predictive_gradient': 0.5,  # Based on historical patterns
        'access_granularity_index': obj.size,  # Based on size
        'priority_inversion_flag': False,  # Initially false
        'predictive_cascade_timestamp': cache_snapshot.access_count  # Current time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After eviction, the Temporal-Contextual Scores of remaining objects are adjusted for fairness, the Predictive Gradient is recalibrated using the evicted object's metadata, the Access Granularity Index remains unchanged, the Priority Inversion Flags are updated to ensure fairness, and the Predictive Cascade Timestamps are adjusted to maintain relative recency.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_metadata = metadata.pop(evicted_obj.key)

    for key, obj_metadata in metadata.items():
        obj_metadata['temporal_contextual_score'] *= 0.9  # Adjust for fairness
        obj_metadata['predictive_gradient'] += evicted_metadata['predictive_gradient'] * 0.1  # Recalibrate
        obj_metadata['priority_inversion_flag'] = False  # Update for fairness
        obj_metadata['predictive_cascade_timestamp'] = cache_snapshot.access_count  # Adjust timestamp
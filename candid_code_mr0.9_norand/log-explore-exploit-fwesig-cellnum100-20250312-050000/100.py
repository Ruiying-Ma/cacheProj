# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict

# Put tunable constant parameters below
GRADIENT_WEIGHT = 0.1
PRUNING_DEFAULT_PROBABILITY = 0.5
LATENCY_CASCADE_BASELINE = 1.0
PARSIMONY_BASELINE_UTILITY = 1.0

# Put the metadata specifically maintained by the policy below. The policy maintains a gradient score for each cache entry, a predictive pruning score based on access patterns, a latency cascade index to track multi-level access delays, and a parsimony index to measure the minimal utility of each entry.
gradient_scores = defaultdict(float)  # Tracks gradient scores for each object key
pruning_scores = defaultdict(float)   # Tracks predictive pruning scores for each object key
latency_cascade = defaultdict(float)  # Tracks latency cascade index for each object key
parsimony_index = defaultdict(float)  # Tracks parsimony index for each object key

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim by combining the gradient score and parsimony index to identify entries with low utility and low future access probability, while factoring in the latency cascade to avoid evicting entries critical to reducing multi-level delays.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_score = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        # Combine gradient score, parsimony index, and latency cascade
        score = gradient_scores[key] + parsimony_index[key] - latency_cascade[key]
        if score < min_score:
            min_score = score
            candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    On a cache hit, the gradient score is incrementally adjusted using a weighted gradient synchronization mechanism, the predictive pruning score is updated to reflect the likelihood of future hits, and the latency cascade index is recalibrated to account for the reduced delay.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key

    # Incrementally adjust the gradient score
    gradient_scores[key] += GRADIENT_WEIGHT

    # Update the predictive pruning score
    pruning_scores[key] = min(1.0, pruning_scores[key] + 0.1)

    # Recalibrate the latency cascade index
    latency_cascade[key] = max(0.0, latency_cascade[key] - 0.1)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the gradient score is initialized based on recent access trends, the predictive pruning score is seeded with a default probability, the latency cascade index is updated to reflect the new entry's impact on delays, and the parsimony index is set to a baseline utility value.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key

    # Initialize the gradient score
    gradient_scores[key] = GRADIENT_WEIGHT

    # Seed the predictive pruning score with a default probability
    pruning_scores[key] = PRUNING_DEFAULT_PROBABILITY

    # Update the latency cascade index
    latency_cascade[key] = LATENCY_CASCADE_BASELINE

    # Set the parsimony index to a baseline utility value
    parsimony_index[key] = PARSIMONY_BASELINE_UTILITY

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an entry, the gradient scores of remaining entries are normalized to maintain balance, the predictive pruning scores are adjusted to reflect the removal's impact on future predictions, and the latency cascade index is recalculated to account for the change in cache composition.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key

    # Remove metadata for the evicted object
    if evicted_key in gradient_scores:
        del gradient_scores[evicted_key]
    if evicted_key in pruning_scores:
        del pruning_scores[evicted_key]
    if evicted_key in latency_cascade:
        del latency_cascade[evicted_key]
    if evicted_key in parsimony_index:
        del parsimony_index[evicted_key]

    # Normalize gradient scores of remaining entries
    total_gradient = sum(gradient_scores.values())
    if total_gradient > 0:
        for key in gradient_scores:
            gradient_scores[key] /= total_gradient

    # Adjust predictive pruning scores
    for key in pruning_scores:
        pruning_scores[key] = max(0.0, pruning_scores[key] - 0.05)

    # Recalculate latency cascade index
    for key in latency_cascade:
        latency_cascade[key] = max(0.0, latency_cascade[key] - 0.05)
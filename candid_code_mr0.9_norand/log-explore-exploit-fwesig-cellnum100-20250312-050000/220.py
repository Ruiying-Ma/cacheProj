# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict
import math

# Put tunable constant parameters below
ENTROPY_DECAY_FACTOR = 0.9  # Controls how entropy decays over time
REDUNDANCY_INCREMENT = 1    # Increment for redundancy factor on frequent access
FUSION_SCORE_WEIGHT = 0.5   # Weight for combining entropy and redundancy in fusion score

# Put the metadata specifically maintained by the policy below. The policy maintains a stochastic mapping table that probabilistically associates cache objects with clusters based on access patterns. Each cluster tracks an entropy score representing the variability of access frequencies, and an adaptive redundancy factor that adjusts the number of replicas for high-priority objects. A dynamic fusion score is calculated for each object, combining its cluster entropy and redundancy factor to determine its overall importance.
clusters = defaultdict(lambda: {"entropy": 0, "objects": set()})
object_metadata = {}  # Maps obj.key to {"cluster": cluster_id, "redundancy": int, "fusion_score": float}
access_frequencies = defaultdict(int)  # Tracks access frequencies for objects

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy selects the eviction victim by identifying the object with the lowest dynamic fusion score within the cluster that has the highest entropy. If multiple candidates exist, a stochastic tie-breaking mechanism is used to ensure diversity in eviction decisions.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Identify the cluster with the highest entropy
    highest_entropy_cluster = max(clusters.items(), key=lambda x: x[1]["entropy"])[0]
    cluster_objects = clusters[highest_entropy_cluster]["objects"]

    # Find the object with the lowest dynamic fusion score in the cluster
    lowest_fusion_score = float("inf")
    for obj_key in cluster_objects:
        fusion_score = object_metadata[obj_key]["fusion_score"]
        if fusion_score < lowest_fusion_score:
            lowest_fusion_score = fusion_score
            candid_obj_key = obj_key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the entropy score of the associated cluster is recalculated to reflect the updated access frequency. The redundancy factor of the object is adaptively increased if it is accessed frequently within a short time window, and the dynamic fusion score is updated accordingly.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    obj_key = obj.key
    cluster_id = object_metadata[obj_key]["cluster"]

    # Update access frequency
    access_frequencies[obj_key] += 1

    # Recalculate entropy for the cluster
    cluster_objects = clusters[cluster_id]["objects"]
    total_accesses = sum(access_frequencies[o] for o in cluster_objects)
    entropy = -sum(
        (access_frequencies[o] / total_accesses) * math.log(access_frequencies[o] / total_accesses)
        for o in cluster_objects if access_frequencies[o] > 0
    )
    clusters[cluster_id]["entropy"] = ENTROPY_DECAY_FACTOR * clusters[cluster_id]["entropy"] + (1 - ENTROPY_DECAY_FACTOR) * entropy

    # Update redundancy factor and fusion score
    object_metadata[obj_key]["redundancy"] += REDUNDANCY_INCREMENT
    redundancy = object_metadata[obj_key]["redundancy"]
    fusion_score = FUSION_SCORE_WEIGHT * clusters[cluster_id]["entropy"] + (1 - FUSION_SCORE_WEIGHT) * redundancy
    object_metadata[obj_key]["fusion_score"] = fusion_score

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy assigns it to a cluster based on its initial access pattern using the stochastic mapping table. The cluster's entropy score is updated to account for the new object, and the object's initial dynamic fusion score is calculated based on its predicted importance.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    obj_key = obj.key

    # Assign the object to a cluster (e.g., based on size or other deterministic criteria)
    cluster_id = obj.size % len(clusters) if clusters else 0
    clusters[cluster_id]["objects"].add(obj_key)

    # Initialize metadata for the object
    object_metadata[obj_key] = {
        "cluster": cluster_id,
        "redundancy": 1,
        "fusion_score": 0
    }

    # Update entropy and fusion score
    update_after_hit(cache_snapshot, obj)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an object, the policy recalculates the entropy score of the affected cluster to reflect the reduced variability. The stochastic mapping table is updated to deprioritize the evicted object's cluster for future insertions, and the redundancy factors of remaining objects in the cluster are adjusted if necessary.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key
    cluster_id = object_metadata[evicted_key]["cluster"]

    # Remove the evicted object from the cluster
    clusters[cluster_id]["objects"].remove(evicted_key)
    del object_metadata[evicted_key]
    del access_frequencies[evicted_key]

    # Recalculate entropy for the cluster
    cluster_objects = clusters[cluster_id]["objects"]
    if cluster_objects:
        total_accesses = sum(access_frequencies[o] for o in cluster_objects)
        entropy = -sum(
            (access_frequencies[o] / total_accesses) * math.log(access_frequencies[o] / total_accesses)
            for o in cluster_objects if access_frequencies[o] > 0
        )
        clusters[cluster_id]["entropy"] = ENTROPY_DECAY_FACTOR * clusters[cluster_id]["entropy"] + (1 - ENTROPY_DECAY_FACTOR) * entropy
    else:
        clusters[cluster_id]["entropy"] = 0
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict, deque

# Put tunable constant parameters below
DEFAULT_INITIAL_SCORE = 100  # Initial predictive score for new objects
SCORE_DECAY_FACTOR = 0.9    # Factor to decay scores during recalibration
LATENCY_IMPACT_WEIGHT = 1.0 # Weight for latency propagation impact in eviction decision

# Put the metadata specifically maintained by the policy below. The policy maintains a predictive score for each object based on access patterns (Predictive Calibration), a hierarchical queue structure to group objects by priority tiers (Hierarchical Queuing), a latency propagation map to track the impact of eviction on system latency (Latency Propagation), and contextual tags to stratify objects based on their usage context (Contextual Stratification).
predictive_scores = {}  # Maps object keys to their predictive scores
hierarchical_queue = defaultdict(deque)  # Maps priority tiers to queues of object keys
latency_propagation_map = {}  # Maps object keys to their latency impact
contextual_tags = {}  # Maps object keys to their contextual tags

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy selects the eviction victim by identifying the object with the lowest predictive score within the lowest-priority tier of the hierarchical queue, while also considering the latency propagation map to minimize system-wide latency impact.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Find the lowest-priority tier with objects
    for tier in sorted(hierarchical_queue.keys()):
        if hierarchical_queue[tier]:
            # Find the object with the lowest predictive score in this tier
            min_score = float('inf')
            for key in hierarchical_queue[tier]:
                score = predictive_scores[key] + LATENCY_IMPACT_WEIGHT * latency_propagation_map.get(key, 0)
                if score < min_score:
                    min_score = score
                    candid_obj_key = key
            break
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    After a cache hit, the predictive score of the accessed object is recalibrated based on recent access patterns, its position in the hierarchical queue is adjusted to reflect higher priority, and its contextual tag is updated to capture the current usage context.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    # Recalibrate predictive score
    predictive_scores[key] = predictive_scores.get(key, DEFAULT_INITIAL_SCORE) * SCORE_DECAY_FACTOR
    # Adjust position in hierarchical queue
    for tier, queue in hierarchical_queue.items():
        if key in queue:
            queue.remove(key)
            break
    hierarchical_queue[0].appendleft(key)  # Move to the highest priority tier
    # Update contextual tag
    contextual_tags[key] = f"hit_at_{cache_snapshot.access_count}"

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy assigns an initial predictive score based on observed patterns, places the object in the appropriate tier of the hierarchical queue, and assigns a contextual tag based on the insertion context.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    # Assign initial predictive score
    predictive_scores[key] = DEFAULT_INITIAL_SCORE
    # Place in the appropriate tier of the hierarchical queue
    hierarchical_queue[0].append(key)  # Start in the highest priority tier
    # Assign contextual tag
    contextual_tags[key] = f"inserted_at_{cache_snapshot.access_count}"

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an object, the policy updates the latency propagation map to reflect the impact of the eviction, recalibrates the predictive scores of remaining objects in the same tier, and adjusts the hierarchical queue to maintain balance.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key
    # Update latency propagation map
    latency_propagation_map[evicted_key] = latency_propagation_map.get(evicted_key, 0) + evicted_obj.size
    # Remove evicted object from hierarchical queue
    for tier, queue in hierarchical_queue.items():
        if evicted_key in queue:
            queue.remove(evicted_key)
            break
    # Recalibrate predictive scores of remaining objects in the same tier
    for key in hierarchical_queue[tier]:
        predictive_scores[key] *= SCORE_DECAY_FACTOR
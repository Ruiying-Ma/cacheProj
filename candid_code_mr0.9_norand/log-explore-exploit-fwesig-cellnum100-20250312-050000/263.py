# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict, deque

# Put tunable constant parameters below
TEMPORAL_SCORE_INIT = 50
HEURISTIC_SCORE_INIT = 1.0
LOAD_FACTOR_THRESHOLD = 0.8
GRADIENT_DRIFT_DECAY = 0.9
PRIORITY_INVERSION_PENALTY = 10

# Put the metadata specifically maintained by the policy below. The policy maintains a temporal score, priority inversion flag, access granularity counter, predictive eviction score, frequency counter, hashed index, traversal order list, load factor threshold, causal graph, semantic tags, dynamic heuristic scores, and a gradient drift vector. It also tracks spillover prioritization and dynamically adjusts based on access patterns, cache load, and semantic trends.
metadata = {
    "temporal_scores": {},  # {key: temporal_score}
    "priority_inversion_flags": {},  # {key: bool}
    "access_granularity": {},  # {key: granularity}
    "predictive_eviction_scores": {},  # {key: score}
    "frequency_counters": defaultdict(int),  # {key: frequency}
    "hashed_index": {},  # {key: hash}
    "traversal_order": deque(),  # deque of keys
    "causal_graph": defaultdict(set),  # {key: set(related_keys)}
    "semantic_tags": defaultdict(set),  # {key: set(tags)}
    "heuristic_scores": {},  # {key: score}
    "gradient_drift": {},  # {key: drift}
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    '''
    candid_obj_key = None
    min_score = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        # Calculate combined score
        combined_score = (
            metadata["temporal_scores"].get(key, 0)
            - (PRIORITY_INVERSION_PENALTY if metadata["priority_inversion_flags"].get(key, False) else 0)
            + metadata["predictive_eviction_scores"].get(key, 0)
            - metadata["frequency_counters"].get(key, 0)
            + metadata["heuristic_scores"].get(key, 0)
            - metadata["gradient_drift"].get(key, 0)
        )

        # Prioritize eviction based on combined score
        if combined_score < min_score:
            min_score = combined_score
            candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    '''
    key = obj.key
    metadata["temporal_scores"][key] += 1
    metadata["priority_inversion_flags"][key] = False
    metadata["access_granularity"][key] = obj.size
    metadata["predictive_eviction_scores"][key] = metadata["frequency_counters"][key] / (cache_snapshot.size + 1)
    metadata["frequency_counters"][key] += 1
    metadata["traversal_order"].remove(key)
    metadata["traversal_order"].append(key)
    for related_key in metadata["causal_graph"][key]:
        metadata["causal_graph"][related_key].add(key)
    metadata["heuristic_scores"][key] += metadata["gradient_drift"].get(key, 0)
    metadata["gradient_drift"][key] *= GRADIENT_DRIFT_DECAY

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    '''
    key = obj.key
    metadata["temporal_scores"][key] = TEMPORAL_SCORE_INIT
    metadata["priority_inversion_flags"][key] = False
    metadata["access_granularity"][key] = obj.size
    metadata["predictive_eviction_scores"][key] = 1 / (cache_snapshot.size + 1)
    metadata["frequency_counters"][key] = 1
    metadata["hashed_index"][key] = hash(key)
    metadata["traversal_order"].append(key)
    load_factor = cache_snapshot.size / cache_snapshot.capacity
    if load_factor > LOAD_FACTOR_THRESHOLD:
        metadata["priority_inversion_flags"][key] = True
    metadata["heuristic_scores"][key] = HEURISTIC_SCORE_INIT
    metadata["semantic_tags"][key] = set()  # Placeholder for semantic tags
    metadata["causal_graph"][key] = set()  # Placeholder for causal dependencies
    metadata["gradient_drift"][key] = 0.0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    '''
    evicted_key = evicted_obj.key
    del metadata["temporal_scores"][evicted_key]
    del metadata["priority_inversion_flags"][evicted_key]
    del metadata["access_granularity"][evicted_key]
    del metadata["predictive_eviction_scores"][evicted_key]
    del metadata["frequency_counters"][evicted_key]
    del metadata["hashed_index"][evicted_key]
    metadata["traversal_order"].remove(evicted_key)
    del metadata["causal_graph"][evicted_key]
    del metadata["semantic_tags"][evicted_key]
    del metadata["heuristic_scores"][evicted_key]
    del metadata["gradient_drift"][evicted_key]

    for key in cache_snapshot.cache:
        metadata["priority_inversion_flags"][key] = False
        metadata["causal_graph"][key].discard(evicted_key)
        metadata["gradient_drift"][key] *= GRADIENT_DRIFT_DECAY
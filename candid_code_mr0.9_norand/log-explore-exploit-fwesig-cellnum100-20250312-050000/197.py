# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict
import math

# Put tunable constant parameters below
SHORT_TERM_WINDOW = 10  # Number of accesses for short-term TAS
LONG_TERM_WINDOW = 100  # Number of accesses for long-term TAS
PFS_WEIGHT = 0.7        # Weight for PFS in APW calculation
TAS_WEIGHT = 0.3        # Weight for TAS in APW calculation

# Put the metadata specifically maintained by the policy below. The policy maintains a temporal access score (TAS) for each object, segmented into recursive time windows to capture short-term and long-term access patterns. It also tracks a predictive fusion score (PFS) derived from a lightweight machine learning model that predicts future access likelihood. An adaptive priority weight (APW) is dynamically assigned to each object based on its TAS and PFS fusion.
metadata = {
    "TAS": defaultdict(lambda: {"short_term": 0, "long_term": 0}),
    "PFS": defaultdict(float),
    "APW": defaultdict(float)
}

def calculate_apw(tas, pfs):
    """Helper function to calculate APW based on TAS and PFS."""
    return TAS_WEIGHT * (tas["short_term"] + tas["long_term"]) + PFS_WEIGHT * pfs

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy evicts the object with the lowest adaptive priority weight (APW). In case of ties, it recursively evaluates the TAS within smaller time windows to break the tie, favoring objects with more recent access patterns.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_apw = math.inf
    candidates = []

    # Find the object(s) with the lowest APW
    for key, cached_obj in cache_snapshot.cache.items():
        apw = metadata["APW"][key]
        if apw < min_apw:
            min_apw = apw
            candidates = [key]
        elif apw == min_apw:
            candidates.append(key)

    # Break ties using TAS (favoring more recent access patterns)
    if len(candidates) > 1:
        candidates.sort(key=lambda key: (
            metadata["TAS"][key]["short_term"],
            metadata["TAS"][key]["long_term"]
        ))

    candid_obj_key = candidates[0]
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the TAS is updated to reflect the current time in all relevant time windows, and the PFS is recalculated using the updated access history. The APW is then recomputed by fusing the new TAS and PFS values.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    current_time = cache_snapshot.access_count

    # Update TAS
    metadata["TAS"][key]["short_term"] = current_time % SHORT_TERM_WINDOW
    metadata["TAS"][key]["long_term"] = current_time % LONG_TERM_WINDOW

    # Recalculate PFS (simple heuristic: higher TAS implies higher PFS)
    tas = metadata["TAS"][key]
    metadata["PFS"][key] = (tas["short_term"] + tas["long_term"]) / (SHORT_TERM_WINDOW + LONG_TERM_WINDOW)

    # Recompute APW
    metadata["APW"][key] = calculate_apw(metadata["TAS"][key], metadata["PFS"][key])

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the TAS is initialized with the current time across all time windows, the PFS is set using an initial prediction based on insertion context, and the APW is computed by combining the initial TAS and PFS.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    current_time = cache_snapshot.access_count

    # Initialize TAS
    metadata["TAS"][key]["short_term"] = current_time % SHORT_TERM_WINDOW
    metadata["TAS"][key]["long_term"] = current_time % LONG_TERM_WINDOW

    # Set initial PFS (simple heuristic: based on object size)
    metadata["PFS"][key] = 1 / obj.size

    # Compute APW
    metadata["APW"][key] = calculate_apw(metadata["TAS"][key], metadata["PFS"][key])

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After eviction, the policy recalibrates the PFS model using the remaining objects in the cache to improve future predictions. It also adjusts the TAS segmentation thresholds if eviction patterns suggest a need for finer or coarser granularity.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key

    # Remove metadata for the evicted object
    if evicted_key in metadata["TAS"]:
        del metadata["TAS"][evicted_key]
    if evicted_key in metadata["PFS"]:
        del metadata["PFS"][evicted_key]
    if evicted_key in metadata["APW"]:
        del metadata["APW"][evicted_key]

    # Recalibrate PFS model (simple heuristic: normalize PFS across remaining objects)
    total_pfs = sum(metadata["PFS"].values())
    for key in metadata["PFS"]:
        metadata["PFS"][key] /= total_pfs if total_pfs > 0 else 1

    # Adjust TAS segmentation thresholds (simple heuristic: based on cache size)
    if cache_snapshot.size > 0.9 * cache_snapshot.capacity:
        global SHORT_TERM_WINDOW, LONG_TERM_WINDOW
        SHORT_TERM_WINDOW = max(5, SHORT_TERM_WINDOW - 1)
        LONG_TERM_WINDOW = max(50, LONG_TERM_WINDOW - 5)
    elif cache_snapshot.size < 0.5 * cache_snapshot.capacity:
        SHORT_TERM_WINDOW += 1
        LONG_TERM_WINDOW += 5
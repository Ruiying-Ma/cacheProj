# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict, deque

# Put tunable constant parameters below
GQ_CAPACITY = 100  # Maximum size of the ghost queue
HOT_THRESHOLD = 10  # Frequency threshold for promotion to hot tier
WARM_THRESHOLD = 5  # Frequency threshold for promotion to warm tier

# Put the metadata specifically maintained by the policy below. The policy maintains frequency counters, recency timestamps, access latency, storage stratification tiers (hot, warm, cold), sequential access alignment metadata, a ghost queue (GQ), recursive filter scores, latency balance factors, predictive indices, and a dynamic tuning parameter for workload adaptation.
metadata = {
    "frequency": defaultdict(int),  # Frequency counters
    "recency": {},  # Recency timestamps
    "access_latency": {},  # Access latency
    "tiers": {"hot": set(), "warm": set(), "cold": set()},  # Stratification tiers
    "alignment_metadata": {},  # Sequential access alignment metadata
    "ghost_queue": deque(),  # Ghost queue (GQ)
    "recursive_filter_scores": {},  # Recursive filter scores
    "latency_balance_factors": {},  # Latency balance factors
    "predictive_indices": {},  # Predictive indices
    "dynamic_tuning_param": 1.0,  # Dynamic tuning parameter
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    '''
    candid_obj_key = None
    # Combine stratification tiers, recursive filter scores, latency balance factors, and predictive indices
    candidates = []
    for key, cached_obj in cache_snapshot.cache.items():
        tier_score = 0
        if key in metadata["tiers"]["cold"]:
            tier_score = 1
        elif key in metadata["tiers"]["warm"]:
            tier_score = 2
        elif key in metadata["tiers"]["hot"]:
            tier_score = 3

        score = (
            tier_score * metadata["dynamic_tuning_param"]
            - metadata["frequency"].get(key, 0)
            - metadata["recency"].get(key, 0)
            + metadata["access_latency"].get(key, 0)
            - metadata["predictive_indices"].get(key, 0)
        )
        candidates.append((score, key))

    # Sort candidates by score (lower score is better for eviction)
    candidates.sort()

    # Select the best candidate for eviction
    if candidates:
        candid_obj_key = candidates[0][1]

    # Fallback to LFU-MQ if no suitable candidate
    if candid_obj_key is None:
        lfu_mq_candidates = sorted(
            cache_snapshot.cache.keys(),
            key=lambda k: (metadata["frequency"].get(k, 0), metadata["recency"].get(k, 0)),
        )
        if lfu_mq_candidates:
            candid_obj_key = lfu_mq_candidates[0]

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    '''
    key = obj.key
    metadata["frequency"][key] += 1
    metadata["recency"][key] = cache_snapshot.access_count
    metadata["access_latency"][key] = cache_snapshot.access_count - metadata["recency"].get(key, 0)

    # Adjust tier based on frequency
    if metadata["frequency"][key] >= HOT_THRESHOLD:
        metadata["tiers"]["hot"].add(key)
        metadata["tiers"]["warm"].discard(key)
        metadata["tiers"]["cold"].discard(key)
    elif metadata["frequency"][key] >= WARM_THRESHOLD:
        metadata["tiers"]["warm"].add(key)
        metadata["tiers"]["cold"].discard(key)
    else:
        metadata["tiers"]["cold"].add(key)

    # Update alignment metadata, recursive filter score, latency balance factor, and predictive index
    metadata["alignment_metadata"][key] = cache_snapshot.access_count
    metadata["recursive_filter_scores"][key] = metadata["frequency"][key] / (cache_snapshot.access_count + 1)
    metadata["latency_balance_factors"][key] = metadata["access_latency"][key]
    metadata["predictive_indices"][key] = metadata["frequency"][key] / (cache_snapshot.access_count + 1)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    '''
    key = obj.key
    metadata["frequency"][key] = 1
    metadata["recency"][key] = cache_snapshot.access_count
    metadata["access_latency"][key] = 0
    metadata["tiers"]["cold"].add(key)

    # Initialize alignment metadata, recursive filter score, latency balance factor, and predictive index
    metadata["alignment_metadata"][key] = cache_snapshot.access_count
    metadata["recursive_filter_scores"][key] = 1 / (cache_snapshot.access_count + 1)
    metadata["latency_balance_factors"][key] = 0
    metadata["predictive_indices"][key] = 1 / (cache_snapshot.access_count + 1)

    # Handle ghost queue
    if key in metadata["ghost_queue"]:
        metadata["ghost_queue"].remove(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    '''
    evicted_key = evicted_obj.key
    metadata["ghost_queue"].append(evicted_key)

    # Trim ghost queue if it exceeds capacity
    while len(metadata["ghost_queue"]) > GQ_CAPACITY:
        metadata["ghost_queue"].popleft()

    # Recalibrate stratification thresholds
    metadata["tiers"]["hot"].discard(evicted_key)
    metadata["tiers"]["warm"].discard(evicted_key)
    metadata["tiers"]["cold"].discard(evicted_key)

    # Update alignment metadata, remove recursive filter score, and adjust predictive index
    metadata["alignment_metadata"].pop(evicted_key, None)
    metadata["recursive_filter_scores"].pop(evicted_key, None)
    metadata["latency_balance_factors"].pop(evicted_key, None)
    metadata["predictive_indices"].pop(evicted_key, None)
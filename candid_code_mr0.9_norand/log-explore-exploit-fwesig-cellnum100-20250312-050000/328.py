# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import math
from collections import defaultdict

# Put tunable constant parameters below
INITIAL_SCALING_FACTOR = 1.0
SCALING_FACTOR_INCREMENT = 0.1
SCALING_FACTOR_DECREMENT = 0.05
ENTROPY_THRESHOLD = 0.1
GRADIENT_DECAY = 0.9
REGRESSION_DECAY = 0.9

# Put the metadata specifically maintained by the policy below. The policy maintains a hybrid priority score combining stochastic priority, gradient map trends, and predicted future access probabilities; a contextual synchronization vector tracking workload phases; an adaptive heuristic scaling factor for sensitivity tuning; an entropy score for partition unpredictability; and dynamic partitions grouping objects by access patterns.
metadata = {
    "hybrid_priority": {},  # {obj.key: priority_score}
    "context_vector": defaultdict(float),  # {workload_phase: score}
    "scaling_factor": INITIAL_SCALING_FACTOR,
    "entropy_scores": defaultdict(float),  # {partition_id: entropy_score}
    "partitions": defaultdict(set),  # {partition_id: set(obj.key)}
    "gradient_map": defaultdict(float),  # {obj.key: gradient_trend}
    "regression_predictions": defaultdict(float),  # {obj.key: predicted_access_prob}
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    '''
    candid_obj_key = None
    # Identify the partition with the highest entropy collapse
    max_entropy_partition = max(metadata["entropy_scores"], key=metadata["entropy_scores"].get, default=None)
    if max_entropy_partition is None:
        return None

    # Find the object with the lowest hybrid priority score in the partition
    lowest_priority = math.inf
    for obj_key in metadata["partitions"][max_entropy_partition]:
        if obj_key in cache_snapshot.cache and metadata["hybrid_priority"][obj_key] < lowest_priority:
            lowest_priority = metadata["hybrid_priority"][obj_key]
            candid_obj_key = obj_key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after a cache hit.
    '''
    obj_key = obj.key

    # Refine hybrid priority score
    metadata["hybrid_priority"][obj_key] = (
        metadata["hybrid_priority"].get(obj_key, 0) +
        metadata["gradient_map"].get(obj_key, 0) * GRADIENT_DECAY +
        metadata["regression_predictions"].get(obj_key, 0) * REGRESSION_DECAY
    )

    # Update context vector
    workload_phase = cache_snapshot.access_count % 10  # Example workload phase calculation
    metadata["context_vector"][workload_phase] += 1

    # Slightly increase scaling factor to favor recency
    metadata["scaling_factor"] += SCALING_FACTOR_INCREMENT

    # Recalculate entropy score for the corresponding partition
    partition_id = next((pid for pid, objs in metadata["partitions"].items() if obj_key in objs), None)
    if partition_id is not None:
        metadata["entropy_scores"][partition_id] = calculate_entropy(metadata["partitions"][partition_id])

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    '''
    obj_key = obj.key

    # Initialize hybrid priority score
    metadata["hybrid_priority"][obj_key] = (
        metadata["gradient_map"].get(obj_key, 0) +
        metadata["regression_predictions"].get(obj_key, 0)
    )

    # Update context vector
    workload_phase = cache_snapshot.access_count % 10  # Example workload phase calculation
    metadata["context_vector"][workload_phase] += 1

    # Reset scaling factor to neutral state
    metadata["scaling_factor"] = INITIAL_SCALING_FACTOR

    # Assign object to a dynamic partition
    partition_id = obj_key[:1]  # Example partitioning by first character of key
    metadata["partitions"][partition_id].add(obj_key)

    # Update entropy score for the partition
    metadata["entropy_scores"][partition_id] = calculate_entropy(metadata["partitions"][partition_id])

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    '''
    evicted_key = evicted_obj.key

    # Discard hybrid priority score of the evicted object
    if evicted_key in metadata["hybrid_priority"]:
        del metadata["hybrid_priority"][evicted_key]

    # Adjust context vector to deprioritize patterns associated with the evicted object
    workload_phase = cache_snapshot.access_count % 10  # Example workload phase calculation
    metadata["context_vector"][workload_phase] -= 1

    # Recalibrate scaling factor
    metadata["scaling_factor"] -= SCALING_FACTOR_DECREMENT

    # Update gradient map and regression model to exclude the evicted object
    if evicted_key in metadata["gradient_map"]:
        del metadata["gradient_map"][evicted_key]
    if evicted_key in metadata["regression_predictions"]:
        del metadata["regression_predictions"][evicted_key]

    # Recalculate entropy score for the partition
    partition_id = next((pid for pid, objs in metadata["partitions"].items() if evicted_key in objs), None)
    if partition_id is not None:
        metadata["partitions"][partition_id].remove(evicted_key)
        metadata["entropy_scores"][partition_id] = calculate_entropy(metadata["partitions"][partition_id])

def calculate_entropy(partition):
    '''
    Helper function to calculate entropy of a partition.
    '''
    total_objects = len(partition)
    if total_objects == 0:
        return 0
    probabilities = [1 / total_objects] * total_objects
    return -sum(p * math.log2(p) for p in probabilities)
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import math

# Put tunable constant parameters below
DEFAULT_TEMPORAL_WEIGHT = 0.4
DEFAULT_CONTEXTUAL_WEIGHT = 0.3
DEFAULT_PREDICTIVE_WEIGHT = 0.2
DEFAULT_ENTROPY_WEIGHT = 0.1
ENTROPY_DECAY_RATE = 0.95
ENTROPY_INCREASE_RATE = 1.05

# Put the metadata specifically maintained by the policy below. The policy maintains a fusion score for each object, which is a weighted combination of contextual relevance (based on access patterns and workload type), temporal recency (time since last access), predictive likelihood of future access (using lightweight machine learning models), and an entropy factor that scales based on the variability of access patterns.
metadata = {
    "fusion_scores": {},  # Maps obj.key to its fusion score
    "last_access_time": {},  # Maps obj.key to its last access time
    "predictive_likelihood": {},  # Maps obj.key to its predictive likelihood
    "entropy_factor": 1.0,  # Global entropy factor
    "weights": {  # Weights for fusion score components
        "temporal": DEFAULT_TEMPORAL_WEIGHT,
        "contextual": DEFAULT_CONTEXTUAL_WEIGHT,
        "predictive": DEFAULT_PREDICTIVE_WEIGHT,
        "entropy": DEFAULT_ENTROPY_WEIGHT,
    },
}

def calculate_fusion_score(obj_key, cache_snapshot):
    '''
    Helper function to calculate the fusion score for a given object key.
    '''
    temporal_recency = cache_snapshot.access_count - metadata["last_access_time"].get(obj_key, 0)
    predictive_likelihood = metadata["predictive_likelihood"].get(obj_key, 0.5)  # Default likelihood
    contextual_relevance = 1 / (1 + math.log(1 + temporal_recency))  # Example contextual relevance formula
    entropy_factor = metadata["entropy_factor"]

    weights = metadata["weights"]
    fusion_score = (
        weights["temporal"] * temporal_recency +
        weights["contextual"] * contextual_relevance +
        weights["predictive"] * predictive_likelihood +
        weights["entropy"] * entropy_factor
    )
    return fusion_score

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    '''
    candid_obj_key = None
    min_fusion_score = float('inf')

    for cached_obj_key, cached_obj in cache_snapshot.cache.items():
        fusion_score = calculate_fusion_score(cached_obj_key, cache_snapshot)
        if fusion_score < min_fusion_score:
            min_fusion_score = fusion_score
            candid_obj_key = cached_obj_key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after a cache hit.
    '''
    obj_key = obj.key
    # Update temporal recency
    metadata["last_access_time"][obj_key] = cache_snapshot.access_count

    # Recalibrate predictive likelihood (example: increase likelihood on hit)
    metadata["predictive_likelihood"][obj_key] = min(
        1.0, metadata["predictive_likelihood"].get(obj_key, 0.5) + 0.1
    )

    # Adjust entropy factor based on stability of recent accesses
    metadata["entropy_factor"] *= ENTROPY_DECAY_RATE

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    '''
    obj_key = obj.key
    # Initialize metadata for the new object
    metadata["last_access_time"][obj_key] = cache_snapshot.access_count
    metadata["predictive_likelihood"][obj_key] = 0.5  # Default likelihood

    # Compute initial fusion score
    metadata["fusion_scores"][obj_key] = calculate_fusion_score(obj_key, cache_snapshot)

    # Update entropy factor globally
    metadata["entropy_factor"] *= ENTROPY_INCREASE_RATE

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    '''
    evicted_obj_key = evicted_obj.key
    # Remove metadata for the evicted object
    metadata["fusion_scores"].pop(evicted_obj_key, None)
    metadata["last_access_time"].pop(evicted_obj_key, None)
    metadata["predictive_likelihood"].pop(evicted_obj_key, None)

    # Recalculate entropy factor to reflect reduced variability
    metadata["entropy_factor"] *= ENTROPY_DECAY_RATE

    # Slightly adjust weights to prioritize relevant factors
    total_weight = sum(metadata["weights"].values())
    metadata["weights"]["temporal"] *= 1.01
    metadata["weights"]["contextual"] *= 0.99
    metadata["weights"]["predictive"] *= 1.02
    metadata["weights"]["entropy"] *= 0.98

    # Normalize weights to ensure they sum to 1
    for key in metadata["weights"]:
        metadata["weights"][key] /= total_weight
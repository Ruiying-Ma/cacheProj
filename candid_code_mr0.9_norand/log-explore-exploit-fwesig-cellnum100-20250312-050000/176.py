# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict

# Put tunable constant parameters below
INITIAL_UGS = 1.0
INITIAL_LATENCY = 1.0
INITIAL_ENTROPY = 0.5
DWM_ALPHA = 0.7  # Weight for new objects in DWM
DWM_BETA = 0.3   # Weight for existing objects in DWM

# Put the metadata specifically maintained by the policy below. The policy maintains a Unified Gradient Score (UGS) that combines probabilistic gradient, semantic entropy, and Unified Predictive Score (UPS) using a Dynamic Weight Matrix (DWM). It also tracks an Adaptive Latency Map (ALM), a Contextual Threshold (CT), and archives metadata of evicted objects for future modeling. A Predictive Entropy Map (PEM) is introduced to capture long-term unpredictability trends.
UGS = defaultdict(lambda: INITIAL_UGS)
ALM = defaultdict(lambda: INITIAL_LATENCY)
CT = 0.5
PEM = defaultdict(lambda: INITIAL_ENTROPY)
archived_metadata = {}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy calculates a Composite Eviction Metric (CEM) by combining UGS, ALM, and Latency Harmonization Metrics (LHM) using DWM. Objects with the lowest CEM are prioritized for eviction, with ties broken by semantic entropy and coherence scores. Contextual redundancy is used as a secondary tie-breaker.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_cem = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        # Calculate Composite Eviction Metric (CEM)
        cem = (DWM_ALPHA * UGS[key] + DWM_BETA * ALM[key] + PEM[key])
        
        # Prioritize object with the lowest CEM
        if cem < min_cem or (cem == min_cem and PEM[key] < PEM[candid_obj_key]):
            min_cem = cem
            candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    UGS is recalculated by increasing probabilistic gradient and UPS based on recent access patterns, while semantic entropy is adjusted to reflect predictability. ALM is updated to reflect reduced latency, and CT is recalibrated based on workload trends. PEM is refined using Bayesian inference to capture long-term trends.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key

    # Update UGS to reflect recent access patterns
    UGS[key] += 1.0 / cache_snapshot.access_count

    # Adjust semantic entropy to reflect predictability
    PEM[key] = max(0.0, PEM[key] - 0.1)

    # Update ALM to reflect reduced latency
    ALM[key] = max(0.1, ALM[key] - 0.1)

    # Recalibrate CT based on workload trends
    global CT
    CT = cache_snapshot.hit_count / max(1, cache_snapshot.access_count)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    UGS is initialized using workload trends and predictive modeling, with DWM balancing new and existing objects. ALM is updated with initial latency, CT is adjusted to account for the new object, and PEM is seeded with neutral entropy values. Metadata for the new object is archived for future modeling.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key

    # Initialize UGS using workload trends
    UGS[key] = INITIAL_UGS

    # Update ALM with initial latency
    ALM[key] = INITIAL_LATENCY

    # Adjust CT to account for the new object
    global CT
    CT = cache_snapshot.size / cache_snapshot.capacity

    # Seed PEM with neutral entropy values
    PEM[key] = INITIAL_ENTROPY

    # Archive metadata for future modeling
    archived_metadata[key] = {
        'size': obj.size,
        'insert_time': cache_snapshot.access_count
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    CEM is recalibrated to redistribute latency and access impact, and DWM is adjusted based on observed performance shifts. ALM is updated to remove the evicted object, CT is recalibrated for the reduced cache size, and PEM is refined to reflect changes in cache dynamics. Metadata of the evicted object is archived for future modeling.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key

    # Recalibrate CEM by redistributing latency and access impact
    for key in cache_snapshot.cache:
        ALM[key] += 0.1

    # Adjust DWM based on observed performance shifts
    global DWM_ALPHA, DWM_BETA
    DWM_ALPHA = max(0.1, DWM_ALPHA - 0.01)
    DWM_BETA = min(0.9, DWM_BETA + 0.01)

    # Remove evicted object from ALM
    if evicted_key in ALM:
        del ALM[evicted_key]

    # Recalibrate CT for the reduced cache size
    global CT
    CT = cache_snapshot.size / cache_snapshot.capacity

    # Refine PEM to reflect changes in cache dynamics
    for key in cache_snapshot.cache:
        PEM[key] = min(1.0, PEM[key] + 0.05)

    # Archive metadata of the evicted object
    archived_metadata[evicted_key] = {
        'size': evicted_obj.size,
        'evict_time': cache_snapshot.access_count
    }
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import math

# Put tunable constant parameters below
LATENCY_ADJUSTMENT_FACTOR = 0.1
DEFAULT_TEMPORAL_BRIDGE_THRESHOLD = 10
BASELINE_ADAPTIVE_OVERLAP_SCORE = 1.0
PROBABILISTIC_GRADIENT_INCREMENT = 0.1

# Put the metadata specifically maintained by the policy below. The policy maintains a hybrid metadata structure combining a probabilistic gradient score, a recursive enumeration tree, an adaptive overlap score, a temporal bridge counter, a neural heuristic model, and a latency threshold. These elements collectively track access patterns, predict future access likelihood, and balance short-term and long-term reuse potential.
metadata = {
    "probabilistic_gradient_scores": {},  # {key: score}
    "recursive_enumeration_tree": {},  # {key: parent_key or None}
    "adaptive_overlap_scores": {},  # {key: score}
    "temporal_bridge_counters": {},  # {key: counter}
    "projection_heuristic_scores": {},  # {key: score}
    "latency_threshold": 1.0,  # Global latency threshold
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    '''
    candid_obj_key = None
    # Your code below
    min_score = math.inf
    for key, cached_obj in cache_snapshot.cache.items():
        combined_score = (
            metadata["probabilistic_gradient_scores"].get(key, 0) +
            metadata["adaptive_overlap_scores"].get(key, 0) +
            metadata["temporal_bridge_counters"].get(key, 0) +
            metadata["projection_heuristic_scores"].get(key, 0)
        )
        if combined_score < min_score or (
            combined_score == min_score and metadata["latency_threshold"] > 0
        ):
            min_score = combined_score
            candid_obj_key = key
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    '''
    # Your code below
    key = obj.key
    metadata["probabilistic_gradient_scores"][key] = (
        metadata["probabilistic_gradient_scores"].get(key, 0) + PROBABILISTIC_GRADIENT_INCREMENT
    )
    metadata["recursive_enumeration_tree"][key] = None  # Update access path (simplified)
    metadata["adaptive_overlap_scores"][key] = (
        metadata["adaptive_overlap_scores"].get(key, BASELINE_ADAPTIVE_OVERLAP_SCORE) + 1
    )
    metadata["temporal_bridge_counters"][key] = 0
    metadata["projection_heuristic_scores"][key] = (
        metadata["probabilistic_gradient_scores"][key] * 0.5  # Example recalculation
    )
    hit_rate = cache_snapshot.hit_count / max(1, cache_snapshot.access_count)
    if abs(hit_rate - metadata["latency_threshold"]) > LATENCY_ADJUSTMENT_FACTOR:
        metadata["latency_threshold"] += LATENCY_ADJUSTMENT_FACTOR * (hit_rate - metadata["latency_threshold"])

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    '''
    # Your code below
    key = obj.key
    metadata["probabilistic_gradient_scores"][key] = 0.5  # Example initialization
    metadata["recursive_enumeration_tree"][key] = None  # Add to tree (simplified)
    metadata["adaptive_overlap_scores"][key] = BASELINE_ADAPTIVE_OVERLAP_SCORE
    metadata["temporal_bridge_counters"][key] = DEFAULT_TEMPORAL_BRIDGE_THRESHOLD
    metadata["projection_heuristic_scores"][key] = (
        metadata["probabilistic_gradient_scores"][key] * 0.5  # Example calculation
    )
    metadata["latency_threshold"] += LATENCY_ADJUSTMENT_FACTOR

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    '''
    # Your code below
    evicted_key = evicted_obj.key
    total_score = sum(metadata["probabilistic_gradient_scores"].values())
    for key in cache_snapshot.cache.keys():
        metadata["probabilistic_gradient_scores"][key] /= max(1, total_score)  # Normalize scores
        metadata["temporal_bridge_counters"][key] += 1
        metadata["projection_heuristic_scores"][key] = (
            metadata["probabilistic_gradient_scores"][key] * 0.5  # Example recalibration
        )
    if evicted_key in metadata["recursive_enumeration_tree"]:
        del metadata["recursive_enumeration_tree"][evicted_key]  # Prune tree
    if evicted_key in metadata["adaptive_overlap_scores"]:
        del metadata["adaptive_overlap_scores"][evicted_key]  # Adjust overlap scores
    hit_rate = cache_snapshot.hit_count / max(1, cache_snapshot.access_count)
    if abs(hit_rate - metadata["latency_threshold"]) > LATENCY_ADJUSTMENT_FACTOR:
        metadata["latency_threshold"] += LATENCY_ADJUSTMENT_FACTOR * (hit_rate - metadata["latency_threshold"])
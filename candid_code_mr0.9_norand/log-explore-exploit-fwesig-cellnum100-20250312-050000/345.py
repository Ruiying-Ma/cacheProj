# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import math

# Put tunable constant parameters below
GRADIENT_WEIGHT = 0.4
ENTROPY_WEIGHT = 0.3
PREDICTIVE_ASCENT_WEIGHT = 0.3
INITIAL_CAUSAL_ENTROPY = 100.0
INITIAL_PREDICTIVE_ASCENT = 1.0
INITIAL_GRADIENT_SCORE = 1.0

# Put the metadata specifically maintained by the policy below. The policy maintains a gradient partitioning score for each object, a causal entropy measure to track the uncertainty of access patterns, a predictive ascent value to estimate future access likelihood, and a contextual fusion vector to integrate temporal and spatial access patterns.
metadata = {
    "gradient_scores": {},  # Maps obj.key to its gradient partitioning score
    "causal_entropies": {},  # Maps obj.key to its causal entropy
    "predictive_ascents": {},  # Maps obj.key to its predictive ascent value
    "contextual_fusion_vectors": {},  # Maps obj.key to its contextual fusion vector (simplified as a scalar for this implementation)
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim by selecting the object with the lowest combined score derived from a weighted fusion of gradient partitioning, causal entropy, and predictive ascent values, ensuring that objects with high future access likelihood and contextual relevance are retained.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_combined_score = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        gradient_score = metadata["gradient_scores"].get(key, INITIAL_GRADIENT_SCORE)
        causal_entropy = metadata["causal_entropies"].get(key, INITIAL_CAUSAL_ENTROPY)
        predictive_ascent = metadata["predictive_ascents"].get(key, INITIAL_PREDICTIVE_ASCENT)
        contextual_fusion = metadata["contextual_fusion_vectors"].get(key, 1.0)

        # Calculate the combined score
        combined_score = (
            GRADIENT_WEIGHT * gradient_score +
            ENTROPY_WEIGHT * causal_entropy +
            PREDICTIVE_ASCENT_WEIGHT * predictive_ascent
        ) * contextual_fusion

        # Select the object with the lowest combined score
        if combined_score < min_combined_score:
            min_combined_score = combined_score
            candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the gradient partitioning score is adjusted to reflect the object's recent access frequency, the causal entropy is recalculated to reduce uncertainty, the predictive ascent value is incrementally updated based on the observed access pattern, and the contextual fusion vector is refined using the current access context.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    metadata["gradient_scores"][key] = metadata["gradient_scores"].get(key, INITIAL_GRADIENT_SCORE) + 1
    metadata["causal_entropies"][key] = max(0, metadata["causal_entropies"].get(key, INITIAL_CAUSAL_ENTROPY) - 1)
    metadata["predictive_ascents"][key] = metadata["predictive_ascents"].get(key, INITIAL_PREDICTIVE_ASCENT) + 0.1
    metadata["contextual_fusion_vectors"][key] = 1.0  # Simplified refinement for this implementation

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the gradient partitioning score is initialized based on its initial access frequency, the causal entropy is set to a high value to reflect initial uncertainty, the predictive ascent value is seeded with a baseline prediction, and the contextual fusion vector is initialized using the insertion context.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    metadata["gradient_scores"][key] = INITIAL_GRADIENT_SCORE
    metadata["causal_entropies"][key] = INITIAL_CAUSAL_ENTROPY
    metadata["predictive_ascents"][key] = INITIAL_PREDICTIVE_ASCENT
    metadata["contextual_fusion_vectors"][key] = 1.0  # Simplified initialization for this implementation

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an object, the gradient partitioning scores of remaining objects are rebalanced, the causal entropy of the cache is recalculated to reflect the reduced uncertainty, the predictive ascent values are adjusted to account for the removed object, and the contextual fusion vectors are normalized to maintain consistency.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key
    if evicted_key in metadata["gradient_scores"]:
        del metadata["gradient_scores"][evicted_key]
    if evicted_key in metadata["causal_entropies"]:
        del metadata["causal_entropies"][evicted_key]
    if evicted_key in metadata["predictive_ascents"]:
        del metadata["predictive_ascents"][evicted_key]
    if evicted_key in metadata["contextual_fusion_vectors"]:
        del metadata["contextual_fusion_vectors"][evicted_key]

    # Rebalance remaining metadata
    for key in cache_snapshot.cache.keys():
        metadata["gradient_scores"][key] *= 0.9  # Decay scores slightly
        metadata["causal_entropies"][key] = max(0, metadata["causal_entropies"][key] - 0.5)
        metadata["predictive_ascents"][key] += 0.05
        metadata["contextual_fusion_vectors"][key] = 1.0  # Normalize to maintain consistency
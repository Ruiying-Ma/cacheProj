# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict, deque

# Put tunable constant parameters below
INIT_DYNAMIC_WEIGHT = 1.0
INIT_REWARD_SCORE = 0
CONTEXTUAL_WEIGHT = 0.5  # Weight assigned to contextual relevance in the scoring calculation

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, recency, contextual relevance, dynamic weight, reward score, a feedback log of past eviction decisions, and a predictive model for future access patterns.
# Frequency of access
access_frequency = defaultdict(int)
# Recency of access: maps key to last access time
last_access_time = {}
# Contextual relevance: this could be more complex, for simplicity keeping as a count of relevant operations
contextual_relevance = defaultdict(int)
# Dynamic weight
dynamic_weights = defaultdict(lambda: INIT_DYNAMIC_WEIGHT)
# Reward score
reward_scores = defaultdict(int)
# Feedback log of past eviction decisions
feedback_log = deque(maxlen=100)  # Keeping the last 100 evictions
# Predictive model for future access patterns: Simplified as projected future access frequency
predictive_model = defaultdict(float)

def evict(cache_snapshot, obj):
    candid_obj_key = None
    min_score = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        freq_score = access_frequency[key]
        recency_score = cache_snapshot.access_count - last_access_time[key]
        context_score = contextual_relevance[key]
        dynamic_weight = dynamic_weights[key]
        reward_score = reward_scores[key]
        predictive_score = predictive_model[key]

        combined_score = (freq_score * dynamic_weight + recency_score + context_score * CONTEXTUAL_WEIGHT + reward_score + predictive_score)

        if combined_score < min_score:
            min_score = combined_score
            candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    key = obj.key
    access_frequency[key] += 1
    last_access_time[key] = cache_snapshot.access_count
    contextual_relevance[key] += 1  # Simplification: increment contextual relevance on each hit
    dynamic_weights[key] += 0.1  # Reward for being frequently accessed
    reward_scores[key] += 1
    predictive_model[key] = 0.9 * predictive_model[key] + 0.1 * access_frequency[key]  # Update prediction

def update_after_insert(cache_snapshot, obj):
    key = obj.key
    access_frequency[key] = 1
    last_access_time[key] = cache_snapshot.access_count
    contextual_relevance[key] = 1
    dynamic_weights[key] = INIT_DYNAMIC_WEIGHT
    reward_scores[key] = INIT_REWARD_SCORE
    predictive_model[key] = 1.0  # Assuming new object may be accessed soon

def update_after_evict(cache_snapshot, obj, evicted_obj):
    evicted_key = evicted_obj.key
    
    # Remove metadata for the evicted object
    del access_frequency[evicted_key]
    del last_access_time[evicted_key]
    del contextual_relevance[evicted_key]
    del dynamic_weights[evicted_key]
    del reward_scores[evicted_key]
    del predictive_model[evicted_key]
    
    # Log the eviction decision
    feedback_log.append(evicted_key)
    
    # Example recalculations based on feedback (simplified)
    for key in cache_snapshot.cache.keys():
        dynamic_weights[key] *= 0.99  # Decay the weight slightly
        predictive_model[key] *= 0.99  # Decay predictive score slightly
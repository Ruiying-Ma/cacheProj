# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict
import time

# Put tunable constant parameters below
# Priority weights
ACCESS_FREQ_WEIGHT = 1
PREDICTED_ACCESS_SCORE_WEIGHT = 1
PRIORITY_SCORE_WEIGHT = 1

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, recency, priority score, partition identifier, sequence of access steps, system load metrics, hierarchical structure for access paths, discovery timestamps, last access time, predicted access score, and overall cache usage metrics.
metadata = {
    "access_frequency": defaultdict(int),
    "recency": {},
    "priority_score": {},
    "partition_identifier": {},
    "sequence_of_access_steps": [],
    "system_load_metrics": {},
    "hierarchical_structure": {},
    "discovery_timestamp": {},
    "last_access_time": {},
    "predicted_access_score": {},
    "overall_cache_usage_metrics": {}
}

def get_eviction_candidate(partition, cache_snapshot, obj):
    min_combined_score = float('inf')
    candidate = None
    for key in partition:
        combined_score = (ACCESS_FREQ_WEIGHT * metadata["access_frequency"][key] + 
                          PRIORITY_SCORE_WEIGHT * metadata["priority_score"][key] + 
                          PREDICTED_ACCESS_SCORE_WEIGHT * metadata["predicted_access_score"][key])
        if combined_score < min_combined_score:
            min_combined_score = combined_score
            candidate = key
    return candidate

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy selects the partition with the lowest overall priority and uses a pathfinding algorithm to locate the least recently used path within that partition. It then evicts the item with the lowest combination of access frequency, priority score, and predicted access score. If the cache is nearing capacity, it prioritizes evicting items with the lowest predicted access score.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Your code below
    if cache_snapshot.size + obj.size > cache_snapshot.capacity:
        partitions = defaultdict(list)
        for key, _ in cache_snapshot.cache.items():
            partition_id = metadata["partition_identifier"].get(key, None)
            if partition_id:
                partitions[partition_id].append(key)
        
        min_priority_partition = None
        min_priority_score = float('inf')
        for partition_id, keys in partitions.items():
            partition_priority = sum(metadata['priority_score'][key] for key in keys) / len(keys)
            if partition_priority < min_priority_score:
                min_priority_partition = partition_id
                min_priority_score = partition_priority
        
        if min_priority_partition:
            candid_obj_key = get_eviction_candidate(partitions[min_priority_partition], cache_snapshot, obj)
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy updates the access frequency, recency, priority score, last access time, and predicted access score of the accessed item. It also updates the sequence of access steps, hierarchical structure, discovery timestamp, and system load metrics if necessary.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    metadata["access_frequency"][key] += 1
    metadata["recency"][key] = cache_snapshot.access_count
    metadata["last_access_time"][key] = cache_snapshot.access_count
    metadata["predicted_access_score"][key] = 1 / (cache_snapshot.access_count - metadata["discovery_timestamp"].get(key, cache_snapshot.access_count) + 1)
    metadata["priority_score"][key] = metadata["access_frequency"][key] + metadata["predicted_access_score"][key]
    
    metadata["sequence_of_access_steps"].append(key)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes its access frequency, recency, priority score, last access time, and predicted access score. It places the object in the appropriate partition, initializes its sequence of access steps, updates the hierarchical structure, discovery timestamp, and adjusts system load metrics and overall cache usage metrics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    metadata["access_frequency"][key] = 1
    metadata["recency"][key] = cache_snapshot.access_count
    metadata["last_access_time"][key] = cache_snapshot.access_count
    metadata["predicted_access_score"][key] = 1
    metadata["priority_score"][key] = 1
    metadata["discovery_timestamp"][key] = cache_snapshot.access_count
    
    partition_id = hash(key) % 10  # Example partition assignment
    metadata["partition_identifier"][key] = partition_id
    metadata["sequence_of_access_steps"].append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Following an eviction, the policy removes the metadata of the evicted item from all tracking structures. It recalculates the priority scores of remaining items in the affected partition, updates the sequence of access steps, hierarchical structure, system load metrics, and overall cache usage metrics. It may also adjust the eviction threshold and recalibrate the weights for LRU and LFU strategies based on recent eviction patterns.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Your code below
    evicted_key = evicted_obj.key
    
    # Remove metadata
    if evicted_key in metadata["access_frequency"]:
        del metadata["access_frequency"][evicted_key]
    if evicted_key in metadata["recency"]:
        del metadata["recency"][evicted_key]
    if evicted_key in metadata["priority_score"]:
        del metadata["priority_score"][evicted_key]
    if evicted_key in metadata["partition_identifier"]:
        del metadata["partition_identifier"][evicted_key]
    if evicted_key in metadata["sequence_of_access_steps"]:
        metadata["sequence_of_access_steps"].remove(evicted_key)
    if evicted_key in metadata["discovery_timestamp"]:
        del metadata["discovery_timestamp"][evicted_key]
    if evicted_key in metadata["last_access_time"]:
        del metadata["last_access_time"][evicted_key]
    if evicted_key in metadata["predicted_access_score"]:
        del metadata["predicted_access_score"][evicted_key]

    partition_id = metadata["partition_identifier"].get(evicted_key, None)
    if partition_id:
        partition_keys = [key for key, pid in metadata["partition_identifier"].items() if pid == partition_id]
        total_access_frequency = sum(metadata["access_frequency"][key] for key in partition_keys)
        total_predicted_access_score = sum(metadata["predicted_access_score"][key] for key in partition_keys)

        for key in partition_keys:
            metadata["priority_score"][key] = (
                ACCESS_FREQ_WEIGHT * metadata["access_frequency"][key] / total_access_frequency +
                PREDICTED_ACCESS_SCORE_WEIGHT * metadata["predicted_access_score"][key] / total_predicted_access_score
            )

    # Update overall cache usage metrics
    metadata["overall_cache_usage_metrics"]["last_eviction_time"] = cache_snapshot.access_count
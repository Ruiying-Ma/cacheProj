# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Don't forget to import datetime for timing purposes.
from collections import deque, defaultdict
import time

# Put tunable constant parameters below
DEFAULT_WEIGHT_LRU = 1.0
DEFAULT_WEIGHT_LFU = 1.0
DEFAULT_WEIGHT_MRU = 1.0

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, recency, relationship strength scores, contextual relevance scores, predictive model scores, access timestamps, most recently used flags, dynamic weight vectors for LRU, LFU, and MRU strategies, and an ordered list of items across multiple levels.
cache_metadata = {
    'access_frequency': defaultdict(int),  # LFU metrics
    'recency': {},  # LRU metrics
    'relationship_strength': defaultdict(lambda: defaultdict(float)), 
    'contextual_relevance': defaultdict(float),  
    'predictive_model': defaultdict(float), 
    'access_timestamps': {}, 
    'most_recently_used': set(),  # MRU flag
    'weights': {'LRU': DEFAULT_WEIGHT_LRU, 'LFU': DEFAULT_WEIGHT_LFU, 'MRU': DEFAULT_WEIGHT_MRU},
    'ordered_list': deque(),  
    'levels': defaultdict(deque)   
}

def calculate_composite_score(obj, current_time):
    """
    Helper function to calculate the composite score.
    """
    key = obj.key
    freq_score = cache_metadata['access_frequency'][key]
    recency_score = current_time - cache_metadata['recency'][key]
    predictive_score = cache_metadata['predictive_model'][key]
    contextual_relevance = cache_metadata['contextual_relevance'][key]
    
    composite_score = (
        cache_metadata['weights']['LFU'] * freq_score + 
        cache_metadata['weights']['LRU'] * recency_score + 
        cache_metadata['weights']['MRU'] * (key in cache_metadata['most_recently_used']) + # MRU score part
        predictive_score +
        contextual_relevance
    )
    
    return composite_score

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    '''
    candid_obj_key = None
    
    current_time = cache_snapshot.access_count
    min_composite_score = float('inf')
    
    for cached_key, cached_obj in cache_snapshot.cache.items():
        composite_score = calculate_composite_score(cached_obj, current_time)
        
        if composite_score < min_composite_score:
            min_composite_score = composite_score
            candid_obj_key = cached_key
        elif composite_score == min_composite_score:  
            if cache_metadata['ordered_list'].index(cached_key) < cache_metadata['ordered_list'].index(candid_obj_key):
                candid_obj_key = cached_key
                
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    key = obj.key
    current_time = cache_snapshot.access_count
    
    cache_metadata['access_frequency'][key] += 1
    cache_metadata['recency'][key] = current_time
    cache_metadata['access_timestamps'][key] = current_time
    cache_metadata['predictive_model'][key] += 0.1  
    cache_metadata['contextual_relevance'][key] += 0.05 
    
    cache_metadata['most_recently_used'].add(key)
    cache_metadata['ordered_list'].remove(key)
    cache_metadata['ordered_list'].appendleft(key)  
    
    # Normalizing and adapting weights in a simple way
    total_hits = cache_snapshot.hit_count + cache_snapshot.miss_count
    hit_rate = cache_snapshot.hit_count / total_hits if total_hits else 0
    cache_metadata['weights']['LRU'] = max(0.1, 1.0 - hit_rate)
    cache_metadata['weights']['LFU'] = max(0.1, hit_rate)
    cache_metadata['weights']['MRU'] = 1.0  

def update_after_insert(cache_snapshot, obj):
    key = obj.key
    current_time = cache_snapshot.access_count
    
    cache_metadata['access_frequency'][key] = 1
    cache_metadata['recency'][key] = current_time
    cache_metadata['access_timestamps'][key] = current_time
    cache_metadata['predictive_model'][key] = 0.5  
    cache_metadata['contextual_relevance'][key] = 0.5  
    cache_metadata['most_recently_used'].add(key)
    
    cache_metadata['ordered_list'].appendleft(key)  
    
    if len(cache_metadata['levels'][0]) < 10:  
        cache_metadata['levels'][0].appendleft(key)
    
    cache_metadata['weights']['LRU'] = 1.0  
    cache_metadata['weights']['LFU'] = 1.0
    cache_metadata['weights']['MRU'] = 1.0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    evicted_key = evicted_obj.key
    
    cache_metadata['ordered_list'].remove(evicted_key)
    
    if evicted_key in cache_metadata['levels'][0]:
        cache_metadata['levels'][0].remove(evicted_key)
        if cache_metadata['levels'][1]:  
            promoted_key = cache_metadata['levels'][1].popleft()
            cache_metadata['levels'][0].appendleft(promoted_key)
    
    del cache_metadata['access_frequency'][evicted_key]
    del cache_metadata['recency'][evicted_key]
    del cache_metadata['access_timestamps'][evicted_key]
    del cache_metadata['predictive_model'][evicted_key]
    del cache_metadata['contextual_relevance'][evicted_key]
    cache_metadata['most_recently_used'].discard(evicted_key)
    
    total_misses = cache_snapshot.miss_count
    miss_rate = total_misses / (total_misses + cache_snapshot.hit_count) if total_misses + cache_snapshot.hit_count else 0
    cache_metadata['weights']['LRU'] = max(0.1, miss_rate)  
    cache_metadata['weights']['LFU'] = max(0.1, 1.0 - miss_rate)
    cache_metadata['weights']['MRU'] = 1.0
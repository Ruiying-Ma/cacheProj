# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import numpy as np

# Put tunable constant parameters below
COLLABORATIVE_SCORE_WEIGHT = 0.2
HIERARCHICAL_LEVEL_WEIGHT = 0.2
PREDICTION_SCORE_WEIGHT = 0.2
ACCESS_FREQUENCY_WEIGHT = 0.2
RECENCY_WEIGHT = 0.2

# Put the metadata specifically maintained by the policy below. The policy maintains metadata including access frequency, recency, and a collaborative score shared among multiple caches. It also includes a hierarchical level indicator and a machine learning model's prediction score for each cache entry.
metadata = {
    'access_frequency': {},
    'recency': {},
    'collaborative_score': {},
    'hierarchical_level': {},
    'prediction_score': {}
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy evaluates multiple eviction candidates simultaneously, considering access frequency, recency, collaborative score, hierarchical level, and prediction score. The candidate with the lowest combined score is chosen for eviction.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_score = float('inf')
    
    for key, cached_obj in cache_snapshot.cache.items():
        score = (
            ACCESS_FREQUENCY_WEIGHT * metadata['access_frequency'].get(key, 0) +
            RECENCY_WEIGHT * (cache_snapshot.access_count - metadata['recency'].get(key, 0)) +
            COLLABORATIVE_SCORE_WEIGHT * metadata['collaborative_score'].get(key, 0) +
            HIERARCHICAL_LEVEL_WEIGHT * metadata['hierarchical_level'].get(key, 0) +
            PREDICTION_SCORE_WEIGHT * metadata['prediction_score'].get(key, 0)
        )
        
        if score < min_score:
            min_score = score
            candid_obj_key = key
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    After a cache hit, the access frequency and recency of the accessed entry are updated. The collaborative score is adjusted based on shared metadata from other caches, and the hierarchical level is recalibrated if necessary. The machine learning model is retrained with the new access pattern.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] = metadata['access_frequency'].get(key, 0) + 1
    metadata['recency'][key] = cache_snapshot.access_count
    # Collaborative score and hierarchical level adjustments would depend on external data
    # For simplicity, we assume they are updated externally
    # Retrain the machine learning model with the new access pattern
    # This is a placeholder for the actual ML model update
    metadata['prediction_score'][key] = np.mean(list(metadata['access_frequency'].values()))

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the access frequency is initialized, recency is set to the current time, and the collaborative score is updated based on shared metadata. The hierarchical level is set based on initial access patterns, and the machine learning model is updated with the new entry.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] = 1
    metadata['recency'][key] = cache_snapshot.access_count
    # Collaborative score and hierarchical level are initialized based on external data
    metadata['collaborative_score'][key] = 0  # Placeholder
    metadata['hierarchical_level'][key] = 0  # Placeholder
    # Update the machine learning model with the new entry
    metadata['prediction_score'][key] = np.mean(list(metadata['access_frequency'].values()))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting a victim, the metadata for the evicted entry is removed. The collaborative score is updated to reflect the change, and the hierarchical structure is adjusted if needed. The machine learning model is retrained to improve future predictions.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    key = evicted_obj.key
    if key in metadata['access_frequency']:
        del metadata['access_frequency'][key]
    if key in metadata['recency']:
        del metadata['recency'][key]
    if key in metadata['collaborative_score']:
        del metadata['collaborative_score'][key]
    if key in metadata['hierarchical_level']:
        del metadata['hierarchical_level'][key]
    if key in metadata['prediction_score']:
        del metadata['prediction_score'][key]
    # Collaborative score and hierarchical structure adjustments would depend on external data
    # Retrain the machine learning model to improve future predictions
    # This is a placeholder for the actual ML model update
    if metadata['access_frequency']:
        mean_access_frequency = np.mean(list(metadata['access_frequency'].values()))
        for k in metadata['prediction_score']:
            metadata['prediction_score'][k] = mean_access_frequency
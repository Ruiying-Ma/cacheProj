# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
INITIAL_PRIORITY_SCORE = 1
COLLABORATIVE_SCORE_FACTOR = 0.5
HIERARCHICAL_LEVEL_INITIAL = 1

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, recency, priority score, partition information, hierarchical structure, discovery timestamp, collaborative score, hierarchical level, and a machine learning model's prediction score.
metadata = {
    'access_frequency': {},
    'recency': {},
    'priority_score': {},
    'partition': {},  # Dummy partitioning: only one partition 'default' for simplicity
    'hierarchical_structure': {},
    'discovery_timestamp': {},
    'collaborative_score': {},
    'hierarchical_level': {},
    # Simulated ML model prediction score (constant for simplicity)
    'prediction_score': {}
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy identifies the partition with the lowest overall priority and evaluates multiple eviction candidates within that partition. It uses a pathfinding algorithm to locate the least recently used path and selects the candidate with the lowest combined score of access frequency, priority score, collaborative score, hierarchical level, and prediction score for eviction.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Your code below
    if cache_snapshot.size + obj.size > cache_snapshot.capacity:
        # Select the partition - here we assume a single partition for simplicity
        partition_keys = cache_snapshot.cache.keys()  # All keys as we use single partition

        # Find the victim using a pathfinding approach (all factors weighted)
        lowest_score = float('inf')
        for key in partition_keys:
            current_score = (metadata['access_frequency'][key] + 
                             metadata['priority_score'][key] + 
                             metadata['collaborative_score'][key] +
                             metadata['hierarchical_level'][key] +
                             metadata['prediction_score'][key]) / 5
            # Pathfinding approach: considering recency inversely
            current_score += (cache_snapshot.access_count - metadata['recency'][key])

            if current_score < lowest_score:
                lowest_score = current_score
                candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    The policy updates the access frequency, recency, and priority score of the accessed item. It adjusts the collaborative score based on shared metadata, recalibrates the hierarchical level if necessary, updates the hierarchical structure to reflect the most recent access path, and retrains the machine learning model with the new access pattern.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] += 1
    metadata['recency'][key] = cache_snapshot.access_count
    metadata['priority_score'][key] += 1  # Increment priority score on hit
    metadata['collaborative_score'][key] *= COLLABORATIVE_SCORE_FACTOR
    metadata['hierarchical_level'][key] = HIERARCHICAL_LEVEL_INITIAL  # Recalibrate hierarchical level
    metadata['hierarchical_structure'][key] = 'last_access_path'  # Simulate structural update
    metadata['prediction_score'][key] = 1.0  # Simulating an ML update (constant for simplicity)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    The policy initializes access frequency, sets recency to the current time, assigns initial priority score, updates the collaborative score based on shared metadata, sets the hierarchical level based on initial access patterns, updates the hierarchical structure to include the new item, sets the discovery timestamp to the current time, and updates the machine learning model with the new entry.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] = 1
    metadata['recency'][key] = cache_snapshot.access_count
    metadata['priority_score'][key] = INITIAL_PRIORITY_SCORE
    metadata['partition'][key] = 'default'  # Assuming a single partition for simplicity
    metadata['hierarchical_structure'][key] = 'initial_path'
    metadata['discovery_timestamp'][key] = cache_snapshot.access_count
    metadata['collaborative_score'][key] = 1.0
    metadata['hierarchical_level'][key] = HIERARCHICAL_LEVEL_INITIAL
    metadata['prediction_score'][key] = 1.0  # Initial prediction score (constant for simplicity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    The policy removes the metadata of the evicted item from all tracking structures, recalculates the overall priority of the affected partition, updates the collaborative score to reflect the change, adjusts the hierarchical structure, and retrains the machine learning model to improve future predictions.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key

    # Remove evicted metadata
    for key in ('access_frequency', 'recency', 'priority_score', 'partition', 
                'hierarchical_structure', 'discovery_timestamp', 
                'collaborative_score', 'hierarchical_level', 
                'prediction_score'):
        if evicted_key in metadata[key]:
            del metadata[key][evicted_key]

    # Recalculate overall priority - Simulated as updating collaborative score
    metadata['collaborative_score'][obj.key] = 1.0

    # Adjust hierarchical structure - Simulated update
    metadata['hierarchical_structure']['adjusted_path'] = 'structure_updated'
    
    # Simulate model retraining
    metadata['prediction_score'][obj.key] = 1.0  # Simplistic retraining
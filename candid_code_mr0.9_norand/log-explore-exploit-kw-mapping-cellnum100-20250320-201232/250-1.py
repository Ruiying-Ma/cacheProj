# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import heapq

# Put tunable constant parameters below
NUM_TIERS = 3  # Number of tiers

# Put the metadata specifically maintained by the policy below. The policy maintains metadata including access frequency, recency of access, a machine learning model's prediction score for future access, and a tier level indicator for each cached object.
object_metadata = {}  # {obj_key: {'frequency': int, 'recency': int, 'pred_score': float, 'tier': int}}

class DummyMLModel:
    def predict(self, obj):
        # Dummy implementation of prediction
        return len(obj.key) % 10

ml_model = DummyMLModel()

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim by first considering objects in the lowest tier, prioritizing those with the lowest access frequency and prediction scores. If necessary, it moves to higher tiers, applying the same criteria.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Your code below
    candidates = []

    # Prioritize based on tier, frequency, and prediction score
    for key, cached_obj in cache_snapshot.cache.items():
        meta = object_metadata[key]
        heapq.heappush(candidates, (meta['tier'], meta['frequency'], meta['pred_score'], meta['recency'], key))

    if candidates:
        _, _, _, _, candid_obj_key = heapq.heappop(candidates)
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy updates the access frequency and recency of access for the object, adjusts its tier level if necessary, and recalculates the prediction score using the machine learning model.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    meta = object_metadata[key]
    
    # Update frequency and recency
    meta['frequency'] += 1
    meta['recency'] = cache_snapshot.access_count
    
    # Update tier level (move to higher tier based on frequency)
    if meta['tier'] < NUM_TIERS - 1:
        meta['tier'] += 1
    
    # Update prediction score
    meta['pred_score'] = ml_model.predict(obj)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes its access frequency and recency of access, assigns it to the lowest tier, and calculates its initial prediction score using the machine learning model.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    object_metadata[key] = {
        'frequency': 1,
        'recency': cache_snapshot.access_count,
        'pred_score': ml_model.predict(obj),
        'tier': 0
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an object, the policy updates the tier levels of remaining objects if necessary and may retrain the machine learning model periodically based on the updated access patterns and usage statistics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Your code below
    evicted_key = evicted_obj.key
    if evicted_key in object_metadata:
        del object_metadata[evicted_key]

    # Retrain ML model periodically if required
    # Here we place a placeholder for such logic if needed in the future
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import heapq

# Put tunable constant parameters below
INITIAL_ACCESS_COUNT = 1
ACCESS_DECAY_FACTOR = 0.9

# Put the metadata specifically maintained by the policy below. The policy maintains a machine learning model for access prediction, a hierarchical structure for data organization, local access counters for each cache line, and a global state for distributed decision-making.
global_state = {
    'hierarchy': {},  # dict of {hierarchy_level: set of keys}
    'access_counters': {},  # dict of {key: local access counter}
    'predictions': {}  # dict of {key: predicted future access count}
}

def predict_access_count(obj_key):
    '''Machine learning model to predict future accesses'''
    return global_state['predictions'].get(obj_key, 0)

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy uses the machine learning model to predict future accesses and selects the least likely accessed item from the lowest level of the hierarchy for eviction. Each cache line independently evaluates its eviction probability based on local access patterns and the global state.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None

    # Get the lowest hierarchy level
    lowest_level = min(global_state['hierarchy'].keys(), default=-1)

    if lowest_level != -1:
        # Find the object with the smallest predicted future access count at the lowest level
        least_likely_accessed_obj_key = min(global_state['hierarchy'][lowest_level], 
                                             key=predict_access_count, 
                                             default=None)
        candid_obj_key = least_likely_accessed_obj_key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy updates the machine learning model with the new access pattern, moves the accessed item up in the hierarchy, increments the local access counter for the cache line, and updates the global state to reflect the access.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key

    # Update access counters
    if key in global_state['access_counters']:
        global_state['access_counters'][key] += 1

    # Update hierarchy levels
    current_level = next((level for level, keys in global_state['hierarchy'].items() if key in keys), None)
    if current_level is not None:
        global_state['hierarchy'][current_level].remove(key)
        global_state['hierarchy'].setdefault(current_level + 1, set()).add(key)

    # Update machine learning model predictions for access
    global_state['predictions'][key] = (global_state['predictions'].get(key, 0) + 1) * ACCESS_DECAY_FACTOR

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes the local access counter for the new cache line, places the item at an appropriate level in the hierarchy based on initial access prediction, and updates the global state to include the new item.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key

    # Initialize local access counter
    global_state['access_counters'][key] = INITIAL_ACCESS_COUNT

    # Place the item at an appropriate level in the hierarchy based on initial access prediction
    predicted_access_count = predict_access_count(key)
    initial_level = int(predicted_access_count)  # Could use a more sophisticated hierarchy organization
    global_state['hierarchy'].setdefault(initial_level, set()).add(key)

    # Update the global state to include the new item
    global_state['predictions'][key] = predicted_access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Following an eviction, the policy updates the machine learning model with the eviction decision, removes the evicted item from the hierarchy, resets the local access counter for the evicted cache line, and updates the global state to reflect the eviction.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key

    # Update the machine learning model with the eviction decision
    if evicted_key in global_state['predictions']:
        del global_state['predictions'][evicted_key]

    # Remove the evicted item from the hierarchy
    for level, keys in global_state['hierarchy'].items():
        if evicted_key in keys:
            keys.remove(evicted_key)
            break

    # Reset the local access counter for the evicted cache line
    if evicted_key in global_state['access_counters']:
        del global_state['access_counters'][evicted_key]
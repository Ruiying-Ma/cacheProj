# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
LRU_THRESHOLD = 0.8  # Threshold for considering load balancing between T1 and T2

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, recency timestamp, sensitivity level, validation score, segment load metrics, and a dynamically adjusted priority score. It also tracks overall access patterns and uses two LRU queues (T1 and T2) with dynamic capacities and two FIFO ghost queues (B1 and B2).
metadata = {
    'T1': {},  # Least Recently Used Queue 1
    'T2': {},  # Least Recently Used Queue 2
    'B1': {},  # Ghost Queue 1
    'B2': {},  # Ghost Queue 2
    'T1_capacity': 0,
    'T2_capacity': 0,
    'frequency': {},  # Access frequency
    'recency': {},  # Recency timestamp
    'sensitivity': {},  # Sensitivity level
    'validation_score': {},  # Validation score
    'priority_score': {},  # Priority score
    'segment_load': {}  # Segment load metrics
}

def recalculate_priority(obj_key):
    # Priority calculation based on frequency, recency, sensitivity, and validation score
    freq = metadata['frequency'].get(obj_key, 1)
    rec = metadata['recency'].get(obj_key, 0)
    sensitive = metadata['sensitivity'].get(obj_key, 1)
    valid = metadata['validation_score'].get(obj_key, 0)
    # For simplicity, a linear combination; weights can be adjusted
    return freq * 0.4 - rec * 0.3 + sensitive * 0.2 + valid * 0.1

def evict(cache_snapshot, obj):
    candid_obj_key = None

    # Identify the segment with highest load
    highest_load_segment = 'T1' if len(metadata['T1']) > len(metadata['T2']) else 'T2'

    # Find the object with least priority score in that segment
    if highest_load_segment == 'T1' and metadata['T1']:
        lowest_priority_key = min(metadata['T1'], key=lambda k: metadata['priority_score'][k])
        candid_obj_key = lowest_priority_key
    elif highest_load_segment == 'T2' and metadata['T2']:
        lowest_priority_key = min(metadata['T2'], key=lambda k: metadata['priority_score'][k])
        candid_obj_key = lowest_priority_key
    else:
        raise Exception("Both T1 and T2 are empty, cannot evict.")

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    obj_key = obj.key
    
    # Update access frequency, recency timestamp, validation score, and priority score
    metadata['frequency'][obj_key] = metadata['frequency'].get(obj_key, 0) + 1
    metadata['recency'][obj_key] = cache_snapshot.access_count
    metadata['validation_score'][obj_key] = (metadata['validation_score'].get(obj_key, 0) + 1) % 10  # Example update
    metadata['priority_score'][obj_key] = recalculate_priority(obj_key)
    
    # Update segments
    if obj_key in metadata['T1']:
        metadata['T1'].pop(obj_key)
        metadata['T2'][obj_key] = obj
    elif obj_key in metadata['T2']:
        metadata['T2'].pop(obj_key)
        metadata['T2'][obj_key] = obj  # Move to MRU end

def update_after_insert(cache_snapshot, obj):
    obj_key = obj.key
    
    # Initialize object's metadata
    metadata['frequency'][obj_key] = 1
    metadata['recency'][obj_key] = cache_snapshot.access_count
    metadata['sensitivity'][obj_key] = 1  # Default value
    metadata['validation_score'][obj_key] = 0  # Starting value
    metadata['priority_score'][obj_key] = recalculate_priority(obj_key)

    # Insert into corresponding segment
    if obj_key in metadata['B1']:
        metadata['B1'].pop(obj_key)
        metadata['T2'][obj_key] = obj
        metadata['T2_capacity'] += obj.size
    elif obj_key in metadata['B2']:
        metadata['B2'].pop(obj_key)
        metadata['T2'][obj_key] = obj
        metadata['T2_capacity'] += obj.size
    else:
        metadata['T1'][obj_key] = obj
        metadata['T1_capacity'] += obj.size
    
    # Adjust segments dynamically
    if metadata['T2_capacity'] + metadata['T1_capacity'] > cache_snapshot.capacity:
        if metadata['T2_capacity'] > cache_snapshot.capacity * LRU_THRESHOLD:
            evict(cache_snapshot, obj)  # Evict from T2 if over-capacity
        else:
            evict(cache_snapshot, obj)  # Evict from T1 otherwise

def update_after_evict(cache_snapshot, obj, evicted_obj):
    evicted_key = evicted_obj.key
    
    # Update segment load metrics and recalculate validation scores
    if evicted_key in metadata['T1']:
        metadata['T1'].pop(evicted_key)
        metadata['T1_capacity'] -= evicted_obj.size
        metadata['B1'][evicted_key] = evicted_obj
    elif evicted_key in metadata['T2']:
        metadata['T2'].pop(evicted_key)
        metadata['T2_capacity'] -= evicted_obj.size
        metadata['B2'][evicted_key] = evicted_obj
    
    # Remove redundant objects from B1 or B2 if full
    if len(metadata['B1']) > cache_snapshot.capacity:
        metadata['B1'].pop(next(iter(metadata['B1'])))
    if len(metadata['B2']) > cache_snapshot.capacity:
        metadata['B2'].pop(next(iter(metadata['B2'])))
    
    # Update overall access patterns
    for key in metadata['T1']:
        metadata['validation_score'][key] = recalculate_priority(key)
    for key in metadata['T2']:
        metadata['validation_score'][key] = recalculate_priority(key)
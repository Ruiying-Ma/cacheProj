# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

from collections import defaultdict
import math

# Put tunable constant parameters below
REWARD_INCREMENT = 1
INITIAL_REWARD = 1
INITIAL_PROBABILITY = 0.5
DECAY_FACTOR = 0.9

# Put the metadata specifically maintained by the policy below. The policy maintains a reward score for each cache entry, a probability estimate of future accesses, and a future state prediction model. It also tracks the historical access patterns to adaptively adjust the reward and probability estimates.
reward_scores = defaultdict(lambda: INITIAL_REWARD)
probability_estimates = defaultdict(lambda: INITIAL_PROBABILITY)
future_state_model = {}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim by selecting the cache entry with the lowest combined score of reward and probability of future access, as predicted by the future state estimation model.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_score = float('inf')

    for key in cache_snapshot.cache:
        combined_score = reward_scores[key] * probability_estimates[key]
        if combined_score < min_score:
            min_score = combined_score
            candid_obj_key = key
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy increases the reward score of the accessed entry and updates the probability estimate based on the latest access pattern. The future state prediction model is also refined using the new access data.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    reward_scores[obj.key] += REWARD_INCREMENT
    access_pattern = cache_snapshot.access_count
    future_state_model[obj.key] = access_pattern
    probability_estimates[obj.key] = 1 - math.exp(-(access_pattern - reward_scores[obj.key])) * DECAY_FACTOR

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes its reward score and probability estimate based on historical access patterns of similar objects. The future state prediction model is updated to include the new object.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    reward_scores[obj.key] = INITIAL_REWARD
    probability_estimates[obj.key] = INITIAL_PROBABILITY
    future_state_model[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an entry, the policy adjusts the reward scores and probability estimates of the remaining entries to reflect the new cache state. The future state prediction model is updated to account for the eviction.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    del reward_scores[evicted_obj.key]
    del probability_estimates[evicted_obj.key]
    del future_state_model[evicted_obj.key]

    # Adjust remaining entries
    for key in cache_snapshot.cache:
        access_pattern = cache_snapshot.access_count
        future_state_model[key] = access_pattern
        probability_estimates[key] = 1 - math.exp(-(access_pattern - reward_scores[key])) * DECAY_FACTOR
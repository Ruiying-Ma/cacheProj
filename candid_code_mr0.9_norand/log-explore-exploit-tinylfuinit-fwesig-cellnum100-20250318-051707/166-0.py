# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import collections
import math

# Put tunable constant parameters below
LATENCY_IMPORTANCE = 1
DISK_SCHEDULE_IMPORTANCE = 1
DATA_LOCALITY_IMPORTANCE = 1
WRITE_STATUS_CLEAN = 1
WRITE_STATUS_DIRTY = 0

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, last access time, data coherence state, bandwidth usage statistics, memory allocation size, disk scheduling priorities, LRU queue position, write status, data locality score, latency impact score, and a circular pointer for each cache line.
cache_metadata = {
    'access_frequency': {},
    'last_access_time': {},
    'coherence_state': {},
    'write_status': {},
    'data_locality_score': {},
    'latency_impact_score': {},
    'disk_scheduling_priorities': {},
    'lru_position': collections.deque(),
    'pointer': 0
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy uses the circular pointer to traverse the cache and calculates a composite score for each entry based on access frequency, last access time, data coherence state, bandwidth usage, memory allocation size, disk scheduling priorities, write status, data locality score, and latency impact score. It evicts the entry with the lowest composite score, with a preference for clean entries and considering disk scheduling priorities.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_score = float('inf')
    
    cache_items = list(cache_snapshot.cache.values())
    start_pointer = cache_metadata['pointer']
    
    for i in range(len(cache_items)):
        index = (start_pointer + i) % len(cache_items)
        item = cache_items[index]
        
        access_frequency = cache_metadata['access_frequency'].get(item.key, 0)
        last_access_time = cache_metadata['last_access_time'].get(item.key, 0)
        data_coherence_state = cache_metadata['coherence_state'].get(item.key, 0)
        write_status = cache_metadata['write_status'].get(item.key, WRITE_STATUS_CLEAN)
        data_locality_score = cache_metadata['data_locality_score'].get(item.key, 0)
        latency_impact_score = cache_metadata['latency_impact_score'].get(item.key, 0)
        disk_scheduling_priorities = cache_metadata['disk_scheduling_priorities'].get(item.key, 0)
        
        composite_score = (
            - access_frequency
            + (cache_snapshot.access_count - last_access_time)
            + data_coherence_state
            + (WRITE_STATUS_DIRTY - write_status) * LATENCY_IMPORTANCE
            + data_locality_score * DATA_LOCALITY_IMPORTANCE
            + latency_impact_score
            + disk_scheduling_priorities * DISK_SCHEDULE_IMPORTANCE
        )
        
        if composite_score < min_score or (composite_score == min_score and write_status == WRITE_STATUS_CLEAN):
            min_score = composite_score
            candid_obj_key = item.key
    
    cache_metadata['pointer'] = (start_pointer + 1) % max(1, len(cache_items))
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Immediately after a hit, the policy increments the access frequency, updates the last access time to the current time, recalculates the data locality score and latency impact score, updates the coherence status if necessary, adjusts disk scheduling priorities, moves the hit object to the most-recently-used end of the LRU queue, and keeps the pointer at its current position.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    current_time = cache_snapshot.access_count
    
    # Update frequencies and times
    cache_metadata['access_frequency'][obj.key] += 1
    cache_metadata['last_access_time'][obj.key] = current_time
    
    # Update other metrics -- Placeholders for updates, replace with actual updates needed
    cache_metadata['data_locality_score'][obj.key] += 1
    cache_metadata['latency_impact_score'][obj.key] += 1
    
    # Update coherence status and disk scheduling priorities if necessary
    cache_metadata['coherence_state'][obj.key] = 0  # Update as needed
    cache_metadata['disk_scheduling_priorities'][obj.key] += 1  # Update as needed
    
    # LRU Queue adjustments
    if obj.key in cache_metadata['lru_position']:
        cache_metadata['lru_position'].remove(obj.key)
    cache_metadata['lru_position'].append(obj.key)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    Immediately after inserting a new object, the policy initializes the access frequency to 1, sets the last access time to the current time, sets the initial memory allocation size and coherence status, marks the write status as clean, calculates initial data locality and latency impact scores, assigns a disk scheduling priority based on expected usage patterns, places the object at the current pointer location without moving the pointer, and moves the object to the most-recently-used end of the LRU queue.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    cache_metadata['access_frequency'][obj.key] = 1
    cache_metadata['last_access_time'][obj.key] = current_time
    cache_metadata['coherence_state'][obj.key] = 0  # Initial state, update as needed
    cache_metadata['write_status'][obj.key] = WRITE_STATUS_CLEAN
    cache_metadata['data_locality_score'][obj.key] = 1  # Initial score, update as needed
    cache_metadata['latency_impact_score'][obj.key] = 1  # Initial score, update as needed
    cache_metadata['disk_scheduling_priorities'][obj.key] = 1  # Initial priority, update as needed
   
    # LRU Queue adjustments
    cache_metadata['lru_position'].append(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Immediately after evicting a victim, the policy removes all associated metadata for the evicted entry, adjusts the data locality and latency impact scores of remaining entries if influenced, updates disk scheduling priorities for remaining objects, removes the evicted object from the LRU queue, and keeps the pointer at its current position.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Remove metadata for evicted entry
    evicted_key = evicted_obj.key
    if evicted_key in cache_metadata['access_frequency']:
        del cache_metadata['access_frequency'][evicted_key]
    if evicted_key in cache_metadata['last_access_time']:
        del cache_metadata['last_access_time'][evicted_key]
    if evicted_key in cache_metadata['coherence_state']:
        del cache_metadata['coherence_state'][evicted_key]
    if evicted_key in cache_metadata['write_status']:
        del cache_metadata['write_status'][evicted_key]
    if evicted_key in cache_metadata['data_locality_score']:
        del cache_metadata['data_locality_score'][evicted_key]
    if evicted_key in cache_metadata['latency_impact_score']:
        del cache_metadata['latency_impact_score'][evicted_key]
    if evicted_key in cache_metadata['disk_scheduling_priorities']:
        del cache_metadata['disk_scheduling_priorities'][evicted_key]

    if evicted_key in cache_metadata['lru_position']:
        cache_metadata['lru_position'].remove(evicted_key)

    # Adjust scores and priorities of remaining entries if necessary -- Placeholder
    # No need to update pointer, as per policy description.
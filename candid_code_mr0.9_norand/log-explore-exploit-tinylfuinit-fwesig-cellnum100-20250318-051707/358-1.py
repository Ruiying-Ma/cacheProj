# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import time

# Put tunable constant parameters below
COMPOSITE_WEIGHT_FREQ = 1.0
COMPOSITE_WEIGHT_TIME = 1.0
COMPOSITE_WEIGHT_FETCH_COST = 1.0
COMPOSITE_WEIGHT_LATENCY_IMPACT = 1.0
COMPOSITE_WEIGHT_REPLICATION = 1.0

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, last access timestamp, fetch cost score, write-back status, latency impact score, replication factor, load distribution, and queue position (FIFO or LRU).
metadata = {
    'access_frequency': {},
    'last_access_timestamp': {},
    'fetch_cost_score': {},
    'write_back_status': {},
    'latency_impact_score': {},
    'replication_factor': {},
    'load_distribution': {},
    'fifo_queue': [],
    'lru_queue_L1': [],
    'lru_queue_L2': [],
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy first checks the FIFO queue for an object with zero frequency and evicts it. If no such object is found, it calculates a composite weighted score based on low access frequency, oldest timestamp, high fetch cost, high latency impact, and low replication factor, ensuring balanced load across cache nodes, and evicts the object with the lowest score.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Check FIFO queue for zero frequency objects
    for key in metadata['fifo_queue']:
        if metadata['access_frequency'][key] == 0:
            candid_obj_key = key
            metadata['fifo_queue'].remove(key)
            break
    if candid_obj_key is None:
        # No zero frequency object found, calculate composite scores
        min_score = float('inf')
        for key, _obj in cache_snapshot.cache.items():
            score = (
                COMPOSITE_WEIGHT_FREQ * metadata['access_frequency'][key] +
                COMPOSITE_WEIGHT_TIME * (time.time() - metadata['last_access_timestamp'][key]) +
                COMPOSITE_WEIGHT_FETCH_COST * metadata['fetch_cost_score'][key] +
                COMPOSITE_WEIGHT_LATENCY_IMPACT * metadata['latency_impact_score'][key] +
                COMPOSITE_WEIGHT_REPLICATION * metadata['replication_factor'][key]
            )
            if score < min_score:
                min_score = score
                candid_obj_key = key
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    After a cache hit, the policy increments the access frequency, updates the last access timestamp to the current time, slightly decreases the fetch cost score, recalculates the latency impact score based on recent access patterns, adjusts the load distribution metadata, and moves the object to the most-recently-used end of the next higher LRU queue.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    current_time = cache_snapshot.access_count

    metadata['access_frequency'][key] += 1
    metadata['last_access_timestamp'][key] = current_time
    metadata['fetch_cost_score'][key] *= 0.95  # Slightly decrease fetch cost score
    metadata['latency_impact_score'][key] = calculate_latency_impact(key)  # Recalculate impact score

    # Move to higher LRU queue if not already in highest
    if key in metadata['lru_queue_L1']:
        metadata['lru_queue_L1'].remove(key)
        metadata['lru_queue_L2'].append(key)
    elif key in metadata['lru_queue_L2']:
        metadata['lru_queue_L2'].remove(key)
    metadata['lru_queue_L2'].append(key)  # Move to MRU end of L2

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes the access frequency to 1, sets the last access timestamp to the current time, calculates the fetch cost score, marks the write-back status as clean, assigns an initial latency impact score, assigns an initial replication factor, updates the load distribution, and places the object at the most-recently-used end of L1.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    current_time = cache_snapshot.access_count

    metadata['access_frequency'][key] = 1
    metadata['last_access_timestamp'][key] = current_time
    metadata['fetch_cost_score'][key] = calculate_initial_fetch_cost(obj)
    metadata['write_back_status'][key] = 'clean'
    metadata['latency_impact_score'][key] = calculate_initial_latency_impact(obj)
    metadata['replication_factor'][key] = calculate_initial_replication_factor(obj)
    update_load_distribution()

    metadata['fifo_queue'].append(key)  # Add to FIFO queue initially
    metadata['lru_queue_L1'].append(key)  # Add to MRU end of L1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting the victim, the policy removes all associated metadata for the evicted entry, adjusts the latency impact scores of remaining entries, recalculates the load distribution to ensure balance, adjusts the replication factors of remaining objects, and moves objects behind the evicted object in the FIFO queue one step forward to fill the vacancy.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    key = evicted_obj.key

    for meta_dict in [metadata['access_frequency'], metadata['last_access_timestamp'], metadata['fetch_cost_score'],
                      metadata['write_back_status'], metadata['latency_impact_score'], metadata['replication_factor']]:
        del meta_dict[key]
    
    metadata['fifo_queue'].remove(key)
    for queue in [metadata['lru_queue_L1'], metadata['lru_queue_L2']]:
        if key in queue:
            queue.remove(key)
    
    update_load_distribution()

def calculate_initial_fetch_cost(obj):
    # Placeholder function for calculating initial fetch cost score
    return obj.size / 1024

def calculate_initial_latency_impact(obj):
    # Placeholder function for calculating initial latency impact score
    return obj.size / 512

def calculate_initial_replication_factor(obj):
    # Placeholder function for calculating initial replication factor
    return 1

def calculate_latency_impact(key):
    # Placeholder function for recalculating latency impact score
    return metadata['access_frequency'][key] / 10

def update_load_distribution():
    # Placeholder function to adjust/load balance the metadata['load_distribution']
    pass
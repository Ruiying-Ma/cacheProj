# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

import time

# Put tunable constant parameters below
COMPRESS_SIZE = 0.8  # Example compression ratio
BANDWIDTH_USAGE = 1.2  # Example bandwidth usage parameter
DEDUPE_REF_WEIGHT = 1.0  # Example deduplication reference weight

# Put the metadata specifically maintained by the policy below. 
# The policy maintains metadata for each cache entry including 
# access frequency, last access time, compressed size, and deduplication references. 
# Additionally, it tracks overall cache compression ratio and bandwidth usage statistics.
metadata = {
    "access_frequency": {},
    "last_access_time": {},
    "compressed_size": {},
    "deduplication_references": {},
    "compression_ratio": COMPRESS_SIZE,
    "bandwidth_usage": BANDWIDTH_USAGE
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim based on a weighted score 
    that considers the entry's access frequency, last access time, compressed size, 
    and deduplication references. Entries with lower access frequency, 
    older last access time, larger compressed size, and fewer deduplication 
    references are more likely to be evicted.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_score = float('inf')

    for key in cache_snapshot.cache:
        access_frequency = metadata['access_frequency'].get(key, 0)
        last_access_time = metadata['last_access_time'].get(key, 0)
        compressed_size = metadata['compressed_size'].get(key, cache_snapshot.cache[key].size * COMPRESS_SIZE)
        dedupe_references = metadata['deduplication_references'].get(key, 1)
        
        score = (
            access_frequency * -1 + 
            last_access_time * -1 + 
            compressed_size + 
            dedupe_references * DEDUPE_REF_WEIGHT
        )

        if score < min_score:
            min_score = score
            candid_obj_key = key
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy updates the access frequency and last access time for the accessed entry. 
    It also recalculates the overall cache compression ratio and adjusts bandwidth usage statistics if necessary.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] = metadata['access_frequency'].get(key, 0) + 1
    metadata['last_access_time'][key] = cache_snapshot.access_count

    # Update other statistics (compression ratio and bandwidth are example trackers)
    metadata['compression_ratio'] = COMPRESS_SIZE  # Example calculation
    metadata['bandwidth_usage'] = BANDWIDTH_USAGE  # Example calculation

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy updates the access frequency and last access time for the new entry, 
    calculates its compressed size, and checks for deduplication opportunities. It then updates the overall cache 
    compression ratio and bandwidth usage statistics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] = 1
    metadata['last_access_time'][key] = cache_snapshot.access_count
    metadata['compressed_size'][key] = obj.size * COMPRESS_SIZE
    metadata['deduplication_references'][key] = 0  # Example, real dedupe lookup needed

    # Update statistics 
    metadata['compression_ratio'] = COMPRESS_SIZE  # Example calculation
    metadata['bandwidth_usage'] = BANDWIDTH_USAGE  # Example calculation

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Following an eviction, the policy removes the metadata of the evicted entry, recalculates the overall cache 
    compression ratio, and adjusts bandwidth usage statistics. It also updates deduplication references for any 
    remaining entries that were linked to the evicted entry.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    key = evicted_obj.key
    if key in metadata['access_frequency']:
        del metadata['access_frequency'][key]
    if key in metadata['last_access_time']:
        del metadata['last_access_time'][key]
    if key in metadata['compressed_size']:
        del metadata['compressed_size'][key]
    if key in metadata['deduplication_references']:
        del metadata['deduplication_references'][key]
    
    # Recalculate deduplication references (placeholder logic)
    for dedup_key in metadata['deduplication_references']:
        metadata['deduplication_references'][dedup_key] -= 1

    # Update statistics 
    metadata['compression_ratio'] = COMPRESS_SIZE  # Example calculation
    metadata['bandwidth_usage'] = BANDWIDTH_USAGE  # Example calculation
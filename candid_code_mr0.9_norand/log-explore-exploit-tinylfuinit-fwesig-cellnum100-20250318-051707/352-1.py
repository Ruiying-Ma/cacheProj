# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import defaultdict

# Put tunable constant parameters below

# Put the metadata specifically maintained by the policy below. The policy maintains latency measurement, last access time, insertion time, load balancing score, global cache hit rate, dynamic threshold for data retention period, access frequency, write status, data locality score, shard identifiers, consistent hash values, mirror locations, and erasure coding fragments for each cache entry.

metadata = {
    "latency_measurement": {}, # latency_measurement[obj.key]
    "last_access_time": {}, # last_access_time[obj.key]
    "insertion_time": {}, # insertion_time[obj.key]
    "load_balancing_score": {}, # load_balancing_score[obj.key]
    "access_frequency": {}, # access_frequency[obj.key]
    "write_status": {}, # write_status[obj.key] = "clean" or "dirty"
    "data_locality_score": {}, # data_locality_score[obj.key]
    "shard_identifier": {}, # shard_identifier[obj.key]
    "consistent_hash_value": {}, # consistent_hash_value[obj.key]
    "mirror_locations": {}, # mirror_locations[obj.key]
    "erasure_coding_fragments": {}, # erasure_coding_fragments[obj.key]
    "global_cache_hit_rate": 0,
    "dynamic_retention_period": 100, # example threshold
    "shard_access_timestamp": defaultdict(int),
    "shard_frequency_count": defaultdict(int)
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim by first identifying entries with the highest latency measurements and lowest load balancing scores. Among these, it selects the entry with the oldest insertion time and lowest data locality score, with a preference for clean entries. It also considers the redundancy provided by mirroring and erasure coding to ensure minimal data loss and optimal load balancing.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    max_latency = float('-inf')
    min_load_score = float('inf')
    
    # First pass to find highest latency measurement and lowest load balancing scores
    for key, cached_obj in cache_snapshot.cache.items():
        if metadata["latency_measurement"][key] > max_latency or (metadata["latency_measurement"][key] == max_latency and metadata["load_balancing_score"][key] < min_load_score):
            max_latency = metadata["latency_measurement"][key]
            min_load_score = metadata["load_balancing_score"][key]
            candid_obj_key = key

    # Among these entries, select the one with the oldest insertion time and lowest data locality score with preference for clean entries
    oldest_insertion_time = float('inf')
    lowest_locality_score = float('inf')

    for key, cached_obj in cache_snapshot.cache.items():
        if (metadata["latency_measurement"][key] == max_latency and metadata["load_balancing_score"][key] == min_load_score):
            if (metadata["insertion_time"][key] < oldest_insertion_time or 
                (metadata["insertion_time"][key] == oldest_insertion_time and metadata["data_locality_score"][key] < lowest_locality_score) or
                (metadata["write_status"][key] == "clean")):
                oldest_insertion_time = metadata["insertion_time"][key]
                lowest_locality_score = metadata["data_locality_score"][key]
                candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    After a cache hit, the policy updates the last access time to the current time, increments the access frequency, recalculates the load balancing score based on frequency of access and latency measurement, updates the data locality score based on the access pattern, updates the access timestamp and frequency count for the shard containing the object, recalculates the consistent hash value, and verifies the integrity of mirrored and erasure coded fragments. The global cache hit rate is also updated.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    metadata["last_access_time"][key] = cache_snapshot.access_count
    metadata["access_frequency"][key] += 1
    metadata["load_balancing_score"][key] = metadata["access_frequency"][key] / (metadata["latency_measurement"][key] + 1)
    metadata["data_locality_score"][key] += 1 # Example update, assumes locality score increases with access
    
    shard_id = metadata["shard_identifier"][key]
    metadata["shard_access_timestamp"][shard_id] = cache_snapshot.access_count
    metadata["shard_frequency_count"][shard_id] += 1

    # For consistent hash and verifying integrity, details depend on implementation specifics
    metadata["consistent_hash_value"][key] = hash(key) % len(metadata["shard_identifier"])
    # Assume integrity check is implicit and not changing the state
    metadata["global_cache_hit_rate"] = cache_snapshot.hit_count / (cache_snapshot.hit_count + cache_snapshot.miss_count)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy sets the initial latency measurement, insertion time, and last access time to the current time, initializes the load balancing score, access frequency to 1, marks the write status as clean, calculates an initial data locality score, adjusts the dynamic threshold for data retention period based on the current cache hit rate, assigns a shard identifier, computes the consistent hash value, creates mirror copies, and generates erasure coding fragments, updating the metadata to reflect these changes.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    current_time = cache_snapshot.access_count
    
    metadata["latency_measurement"][key] = 0 # Initial latency measurement is 0
    metadata["last_access_time"][key] = current_time
    metadata["insertion_time"][key] = current_time
    metadata["load_balancing_score"][key] = 1 # Initial load balancing score
    metadata["access_frequency"][key] = 1
    metadata["write_status"][key] = "clean"
    metadata["data_locality_score"][key] = 0 # Initial data locality score

    metadata["dynamic_retention_period"] = 100 * (1 - metadata["global_cache_hit_rate"]) # Example threshold adjustment

    shard_id = len(metadata["shard_identifier"]) % 10 # Example to assign shard identifier
    metadata["shard_identifier"][key] = shard_id

    metadata["consistent_hash_value"][key] = hash(key) % len(metadata["shard_identifier"])

    # Mirror copies and erasure coding fragments would be implementation specific
    metadata["mirror_locations"][key] = "location_example"
    metadata["erasure_coding_fragments"][key] = "fragments_example"

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Following an eviction, the policy recalculates the global cache hit rate, adjusts the dynamic threshold for data retention period, updates the load balancing scores and data locality scores of remaining entries, removes all associated metadata for the evicted entry including shard identifier, consistent hash value, mirror locations, and erasure coding fragments, and redistributes the remaining objects to maintain balanced shards.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    key = evicted_obj.key
    # Remove all associated metadata for the evicted entry
    del metadata["latency_measurement"][key]
    del metadata["last_access_time"][key]
    del metadata["insertion_time"][key]
    del metadata["load_balancing_score"][key]
    del metadata["access_frequency"][key]
    del metadata["write_status"][key]
    del metadata["data_locality_score"][key]
    del metadata["shard_identifier"][key]
    del metadata["consistent_hash_value"][key]
    del metadata["mirror_locations"][key]
    del metadata["erasure_coding_fragments"][key]

    # Redistribute remaining objects to maintain balanced shards
    # Example redistribution logic
    shard_counts = defaultdict(int)

    for existing_key in metadata["shard_identifier"]:
        shard_id = metadata["shard_identifier"][existing_key]
        shard_counts[shard_id] += 1

    for existing_key in metadata["shard_identifier"]:
        if shard_counts[metadata["shard_identifier"][existing_key]] > 1:
            metadata["shard_identifier"][existing_key] = len(metadata["shard_identifier"]) % 10
            shard_counts[metadata["shard_identifier"][existing_key]] += 1

    # Updates after eviction
    metadata["global_cache_hit_rate"] = cache_snapshot.hit_count / (cache_snapshot.hit_count + cache_snapshot.miss_count)
    metadata["dynamic_retention_period"] = 100 * (1 - metadata["global_cache_hit_rate"]) # Example adjustment

    for remaining_key in cache_snapshot.cache.keys():
        metadata["load_balancing_score"][remaining_key] = metadata["access_frequency"][remaining_key] / (metadata["latency_measurement"][remaining_key] + 1)
        metadata["data_locality_score"][remaining_key] += 1 # Example update, assumes locality score increases with access
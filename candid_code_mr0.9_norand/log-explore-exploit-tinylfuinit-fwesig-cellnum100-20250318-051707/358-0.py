# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import time

# Put tunable constant parameters below
FETCH_COST_DECREASE = 0.95
INITIAL_FETCH_COST = 1.0
INITIAL_LATENCY_IMPACT = 1.0
INITIAL_REPLICATION_FACTOR = 1.0

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, last access timestamp, fetch cost score, write-back status, latency impact score, replication factor, load distribution, and queue position (FIFO or LRU).
policy_metadata = {
    'access_frequency': {},
    'last_access_timestamp': {},
    'fetch_cost_score': {},
    'write_back_status': {},
    'latency_impact_score': {},
    'replication_factor': {},
    'load_distribution': {},
    'queue_position': {}
}
lru_queues = {
    'L1': [],
    'L2': [],
    'L3': [],
    # Add more LRU queues as needed
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy first checks the FIFO queue for an object with zero frequency and evicts it. If no such object is found, it calculates a composite weighted score based on low access frequency, oldest timestamp, high fetch cost, high latency impact, and low replication factor, ensuring balanced load across cache nodes, and evicts the object with the lowest score.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Check FIFO queue for an object with zero frequency
    for key in lru_queues['L1']:
        if policy_metadata['access_frequency'][key] == 0:
            candid_obj_key = key
            break

    if candid_obj_key is None:
        # Compute composite weighted score and select the lowest score
        lowest_score = float('inf')
        for key in cache_snapshot.cache.keys():
            score = (policy_metadata['access_frequency'][key] * 1.0 + 
                     (cache_snapshot.access_count - policy_metadata['last_access_timestamp'][key]) * 1.0 + 
                     policy_metadata['fetch_cost_score'][key] * 1.0 - 
                     policy_metadata['latency_impact_score'][key] * 1.0 + 
                     policy_metadata['replication_factor'][key] * 1.0)
            if score < lowest_score:
                lowest_score = score
                candid_obj_key = key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    After a cache hit, the policy increments the access frequency, updates the last access timestamp to the current time, slightly decreases the fetch cost score, recalculates the latency impact score based on recent access patterns, adjusts the load distribution metadata, and moves the object to the most-recently-used end of the next higher LRU queue.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    policy_metadata['access_frequency'][key] += 1
    policy_metadata['last_access_timestamp'][key] = cache_snapshot.access_count
    policy_metadata['fetch_cost_score'][key] *= FETCH_COST_DECREASE
    policy_metadata['latency_impact_score'][key] = calculate_latency_impact_score(key)
    update_load_distribution()

    # Move to the next higher LRU queue
    move_to_next_lru_queue(key)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes the access frequency to 1, sets the last access timestamp to the current time, calculates the fetch cost score, marks the write-back status as clean, assigns an initial latency impact score, assigns an initial replication factor, updates the load distribution, and places the object at the most-recently-used end of L1.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    policy_metadata['access_frequency'][key] = 1
    policy_metadata['last_access_timestamp'][key] = cache_snapshot.access_count
    policy_metadata['fetch_cost_score'][key] = INITIAL_FETCH_COST
    policy_metadata['write_back_status'][key] = 'clean'
    policy_metadata['latency_impact_score'][key] = INITIAL_LATENCY_IMPACT
    policy_metadata['replication_factor'][key] = INITIAL_REPLICATION_FACTOR
    update_load_distribution()
    lru_queues['L1'].append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting the victim, the policy removes all associated metadata for the evicted entry, adjusts the latency impact scores of remaining entries, recalculates the load distribution to ensure balance, adjusts the replication factors of remaining objects, and moves objects behind the evicted object in the FIFO queue one step forward to fill the vacancy.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    key = evicted_obj.key
    del policy_metadata['access_frequency'][key]
    del policy_metadata['last_access_timestamp'][key]
    del policy_metadata['fetch_cost_score'][key]
    del policy_metadata['write_back_status'][key]
    del policy_metadata['latency_impact_score'][key]
    del policy_metadata['replication_factor'][key]

    update_load_distribution()

    for queue_name, queue in lru_queues.items():
        if key in queue:
            queue.remove(key)
            break

# Auxiliary functions
def calculate_latency_impact_score(key):
    # Placeholder function to calculate the latency impact score
    return 1.0

def update_load_distribution():
    # Placeholder function to update the load distribution
    pass

def move_to_next_lru_queue(key):
    # Placeholder function to move the object to the next higher LRU queue
    for i in range(len(lru_queues), 1, -1):
        queue_name = f"L{i-1}"
        next_queue_name = f"L{i}"
        if key in lru_queues[queue_name]:
            lru_queues[queue_name].remove(key)
            lru_queues[next_queue_name].append(key)
            break
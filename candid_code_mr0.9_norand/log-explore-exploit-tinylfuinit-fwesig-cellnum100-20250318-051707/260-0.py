# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
WEIGHT_ACCESS_FREQUENCY = 1.0
WEIGHT_ACCESS_TIMESTAMP = 1.0
WEIGHT_REPLICATION_FACTOR = 1.0
WEIGHT_CPU_UTILIZATION = 1.0
WEIGHT_MEMORY_USAGE = 1.0
WEIGHT_CACHE_COHERENCE = 1.0
WEIGHT_DISK_IO_SPEED = 1.0

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, last access timestamp, replication factor, load distribution, CPU utilization, memory usage, cache coherence status, disk I/O speed, and a composite score combining these factors.
metadata = {
    "access_frequency": {},
    "last_access_timestamp": {},
    "replication_factor": {},
    "cpu_utilization": {},
    "memory_usage": {},
    "cache_coherence": {},
    "disk_io_speed": {},
    "composite_score": {},
}

def compute_composite_score(obj_key):
    af = metadata["access_frequency"].get(obj_key, 0)
    ts = metadata["last_access_timestamp"].get(obj_key, 0)
    rf = metadata["replication_factor"].get(obj_key, 0)
    cu = metadata["cpu_utilization"].get(obj_key, 0)
    mu = metadata["memory_usage"].get(obj_key, 0)
    cc = metadata["cache_coherence"].get(obj_key, 0)
    dis = metadata["disk_io_speed"].get(obj_key, 0)
    return (
        WEIGHT_ACCESS_FREQUENCY * af -
        WEIGHT_ACCESS_TIMESTAMP * ts +
        WEIGHT_REPLICATION_FACTOR * rf +
        WEIGHT_CPU_UTILIZATION * cu -
        WEIGHT_MEMORY_USAGE * mu -
        WEIGHT_CACHE_COHERENCE * cc -
        WEIGHT_DISK_IO_SPEED * dis
    )

def evict(cache_snapshot, obj):
    candid_obj_key = None
    min_score = float('inf')
    for obj_key, cached_obj in cache_snapshot.cache.items():
        score = metadata["composite_score"].get(obj_key, float('inf'))
        if score < min_score:
            candid_obj_key = obj_key
            min_score = score
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    key = obj.key
    # Increment access frequency
    metadata["access_frequency"][key] = metadata["access_frequency"].get(key, 0) + 1
    # Update the last access timestamp
    metadata["last_access_timestamp"][key] = cache_snapshot.access_count
    # Adjust load distribution (if needed)
    # Update CPU utilization and memory usage (these are placeholders)
    metadata["cpu_utilization"][key] = 0
    metadata["memory_usage"][key] = obj.size
    # Recalculate composite score
    metadata["composite_score"][key] = compute_composite_score(key)

def update_after_insert(cache_snapshot, obj):
    key = obj.key
    # Initialize access frequency
    metadata["access_frequency"][key] = 0
    # Set the current timestamp as the last access time
    metadata["last_access_timestamp"][key] = cache_snapshot.access_count
    # Assign an initial replication factor (placeholder as 1)
    metadata["replication_factor"][key] = 1
    # Update load distribution (if needed)
    # Initialize CPU utilization, memory usage, cache coherence, and disk I/O speed (placeholders)
    metadata["cpu_utilization"][key] = 0
    metadata["memory_usage"][key] = obj.size
    metadata["cache_coherence"][key] = 0
    metadata["disk_io_speed"][key] = 0
    # Calculate the initial composite score
    metadata["composite_score"][key] = compute_composite_score(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    evicted_key = evicted_obj.key
    # Remove the object's metadata
    if evicted_key in metadata["access_frequency"]:
        del metadata["access_frequency"][evicted_key]
        del metadata["last_access_timestamp"][evicted_key]
        del metadata["replication_factor"][evicted_key]
        del metadata["cpu_utilization"][evicted_key]
        del metadata["memory_usage"][evicted_key]
        del metadata["cache_coherence"][evicted_key]
        del metadata["disk_io_speed"][evicted_key]
        del metadata["composite_score"][evicted_key]
    
    # Adjust replication factors of remaining objects if necessary (placeholder logic)
    for key in metadata["replication_factor"]:
        metadata["replication_factor"][key] = max(1, metadata["replication_factor"][key] - 1)
        # Recalculate composite scores for remaining entries if necessary
        metadata["composite_score"][key] = compute_composite_score(key)
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import collections
import hashlib

# Put tunable constant parameters below
WINDOW_CACHE_SIZE = 5  # Size of the window cache (W)
MAIN_CACHE_SIZE = 95  # Size of the main cache (M)
CBF_DECAY_INTERVAL = 100  # Number of evictions after which CBF decays

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, last access time, write status, data locality score, and a Count Bloom Filter (CBF) for frequency estimation.
access_frequency = collections.defaultdict(int)
last_access_time = collections.defaultdict(int)
write_status = collections.defaultdict(lambda: 'clean')
data_locality_score = collections.defaultdict(float)
cbf = collections.defaultdict(int)
eviction_count = 0

window_cache = collections.OrderedDict()

def hash_key(key):
    return int(hashlib.md5(key.encode()).hexdigest(), 16)

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy selects a victim based on a composite score derived from access frequency, last access time, data locality score, and write status, with a preference for clean entries. It uses a two-part cache structure with a small window cache (W) using LRU and a larger main cache (M) with the composite score-based eviction policy. The least recently used object from W is compared with an object from M using CBF frequency, and the higher frequency object is retained.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global eviction_count
    eviction_count += 1

    if len(window_cache) >= WINDOW_CACHE_SIZE:
        lru_key = next(iter(window_cache))
        lru_obj = window_cache[lru_key]
        window_cache.pop(lru_key)
    else:
        lru_key = None
        lru_obj = None

    composite_scores = {}
    for key, cached_obj in cache_snapshot.cache.items():
        composite_scores[key] = (
            access_frequency[key] +
            (cache_snapshot.access_count - last_access_time[key]) +
            data_locality_score[key] +
            (0 if write_status[key] == 'clean' else 1)
        )

    if lru_key:
        composite_scores[lru_key] = (
            access_frequency[lru_key] +
            (cache_snapshot.access_count - last_access_time[lru_key]) +
            data_locality_score[lru_key] +
            (0 if write_status[lru_key] == 'clean' else 1)
        )

    candid_obj_key = min(composite_scores, key=composite_scores.get)

    if lru_key and cbf[hash_key(lru_key)] > cbf[hash_key(candid_obj_key)]:
        candid_obj_key = lru_key

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    After a hit, the policy increments the access frequency, updates the last access time to the current time, recalculates the data locality score, and increases the hit object's frequency in CBF.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    access_frequency[obj.key] += 1
    last_access_time[obj.key] = cache_snapshot.access_count
    data_locality_score[obj.key] = calculate_data_locality_score(obj)
    cbf[hash_key(obj.key)] += 1

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes the access frequency to 1, sets the last access time to the current time, marks the write status as clean, calculates an initial data locality score, and increases the inserted object's frequency in CBF. The object is added to W.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    access_frequency[obj.key] = 1
    last_access_time[obj.key] = cache_snapshot.access_count
    write_status[obj.key] = 'clean'
    data_locality_score[obj.key] = calculate_data_locality_score(obj)
    cbf[hash_key(obj.key)] += 1
    window_cache[obj.key] = obj

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting a victim, the policy removes all associated metadata for the evicted entry, adjusts the data locality scores of remaining entries if influenced, and decays CBF every K evictions.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    del access_frequency[evicted_obj.key]
    del last_access_time[evicted_obj.key]
    del write_status[evicted_obj.key]
    del data_locality_score[evicted_obj.key]
    del cbf[hash_key(evicted_obj.key)]

    if eviction_count % CBF_DECAY_INTERVAL == 0:
        for key in cbf:
            cbf[key] = max(0, cbf[key] - 1)

def calculate_data_locality_score(obj):
    # Placeholder function for calculating data locality score
    # This should be replaced with the actual calculation logic
    return 0.0
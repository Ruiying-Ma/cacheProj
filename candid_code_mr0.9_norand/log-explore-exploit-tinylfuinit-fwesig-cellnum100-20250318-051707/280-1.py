# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
MAX_CLUSTER_ID = 100
INITIAL_TREND_SCORE = 1.0
ANOMALY_THRESHOLD = 0.5
CONTEXT_USAGE_DECAY = 0.9

# Put the metadata specifically maintained by the policy below. The policy maintains metadata including cluster identifiers for data objects, predictive trend scores, anomaly flags, and contextual usage patterns.
metadata = {
    'cluster_id': {},              # Mapping from obj.key to cluster id
    'predictive_trend_score': {},  # Mapping from obj.key to trend score
    'anomaly_flag': {},            # Mapping from obj.key to anomaly flag
    'contextual_usage': {}         # Mapping from obj.key to contextual usage
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim based on a combination of low predictive trend scores, high anomaly flags, and infrequent contextual usage patterns within their respective clusters.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Your code below
    min_score = float('inf')
    
    for key, cached_obj in cache_snapshot.cache.items():
        cluster_id = metadata['cluster_id'][key]
        trend_score = metadata['predictive_trend_score'][key]
        anom_flag = metadata['anomaly_flag'][key]
        usage_pattern = metadata['contextual_usage'][key]
        
        score = trend_score - (anom_flag * ANOMALY_THRESHOLD) + (1 - usage_pattern)
        
        if score < min_score:
            min_score = score
            candid_obj_key = key
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    After a cache hit, the policy updates the cluster identifier to reflect any changes in data clustering, adjusts the predictive trend score based on recent access patterns, and re-evaluates the anomaly flag and contextual usage pattern.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    # Update predictive trend score
    metadata['predictive_trend_score'][key] += 1
    
    # Recalculate anomaly flag
    metadata['anomaly_flag'][key] = int(metadata['predictive_trend_score'][key] > ANOMALY_THRESHOLD)
    
    # Update contextual usage pattern
    metadata['contextual_usage'][key] = min(1.0, metadata['contextual_usage'][key] * CONTEXT_USAGE_DECAY + 1)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy assigns a cluster identifier, initializes the predictive trend score, sets the anomaly flag based on initial analysis, and maps the contextual usage pattern.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    metadata['cluster_id'][key] = key.__hash__() % MAX_CLUSTER_ID
    metadata['predictive_trend_score'][key] = INITIAL_TREND_SCORE
    metadata['anomaly_flag'][key] = int(INITIAL_TREND_SCORE > ANOMALY_THRESHOLD)
    metadata['contextual_usage'][key] = 1.0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting a victim, the policy rebalances the cluster identifiers, recalculates predictive trend scores for remaining objects, updates anomaly flags, and adjusts contextual usage patterns to reflect the removal.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Your code below
    evicted_key = evicted_obj.key
    del metadata['cluster_id'][evicted_key]
    del metadata['predictive_trend_score'][evicted_key]
    del metadata['anomaly_flag'][evicted_key]
    del metadata['contextual_usage'][evicted_key]
    
    for key in cache_snapshot.cache:
        # Rebalance cluster ids if required (can be complex, here we keep it unchanged for simplicity)
        metadata['predictive_trend_score'][key] *= CONTEXT_USAGE_DECAY
        metadata['anomaly_flag'][key] = int(metadata['predictive_trend_score'][key] > ANOMALY_THRESHOLD)
        metadata['contextual_usage'][key] *= CONTEXT_USAGE_DECAY
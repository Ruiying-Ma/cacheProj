# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.

# Put tunable constant parameters below
WEIGHT_ACCESS_FREQUENCY = 1.0
WEIGHT_LAST_ACCESS_TIME = 1.0
WEIGHT_OBJECT_SIZE = 1.0
WEIGHT_PREDICTED_FUTURE_ACCESS = 1.0
WEIGHT_LATENCY_SENSITIVITY = 1.0
WEIGHT_PREDICTIVE_SCORE = 1.0
WEIGHT_ADAPTIVE_PRIORITY = 1.0

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, last access time, object size, predicted future access patterns, latency sensitivity, scalability factor, semantic tags, predictive score, adaptive priority level, and coherence flag.
metadata = {
    'access_frequency': {},
    'last_access_time': {},
    'predicted_future_access': {},
    'latency_sensitivity': {},
    'semantic_tags': {},
    'predictive_score': {},
    'adaptive_priority': {},
    'coherence_flag': {},
    'scalability_factor': 1.0
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim by calculating a weighted score that combines low access frequency, old last access time, large object size, low predicted future access, low latency sensitivity, low predictive score, and low adaptive priority, while ensuring data coherence by checking the coherence flag. The scalability factor adjusts the weight of each component based on current cache size and load.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_score = float('inf')
    
    for key, cached_obj in cache_snapshot.cache.items():
        if not metadata['coherence_flag'][key]:
            continue
        
        score = (
            WEIGHT_ACCESS_FREQUENCY * (1 / (metadata['access_frequency'][key] + 1)) +
            WEIGHT_LAST_ACCESS_TIME * (cache_snapshot.access_count - metadata['last_access_time'][key]) +
            WEIGHT_OBJECT_SIZE * cached_obj.size +
            WEIGHT_PREDICTED_FUTURE_ACCESS * (1 / (metadata['predicted_future_access'][key] + 1)) +
            WEIGHT_LATENCY_SENSITIVITY * (1 / (metadata['latency_sensitivity'][key] + 1)) +
            WEIGHT_PREDICTIVE_SCORE * (1 / (metadata['predictive_score'][key] + 1)) +
            WEIGHT_ADAPTIVE_PRIORITY * (1 / (metadata['adaptive_priority'][key] + 1))
        ) * metadata['scalability_factor']
        
        if score < min_score:
            min_score = score
            candid_obj_key = key
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Immediately after a hit, the policy increments the access frequency, refreshes the last access time to the current time, refines the predicted future access pattern using the machine learning model, reinforces the semantic tags, updates the predictive score based on recent access patterns, increases the adaptive priority, and checks and updates the coherence flag if necessary.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] += 1
    metadata['last_access_time'][key] = cache_snapshot.access_count
    # Assume machine learning model updates are done here
    metadata['predicted_future_access'][key] += 1
    metadata['predictive_score'][key] += 1
    metadata['adaptive_priority'][key] += 1
    metadata['coherence_flag'][key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    Immediately after inserting a new object, the policy initializes the access frequency to 1, sets the last access time to the current time, estimates the initial predicted future access using the machine learning model, records the object's latency sensitivity, assigns semantic tags, initializes the predictive score based on initial access patterns, sets the adaptive priority to a default level, sets the coherence flag to indicate data freshness, and adjusts the scalability factor if the cache size has changed.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    key = obj.key
    metadata['access_frequency'][key] = 1
    metadata['last_access_time'][key] = cache_snapshot.access_count
    # Assume machine learning model estimates are done here
    metadata['predicted_future_access'][key] = 1
    metadata['latency_sensitivity'][key] = 1
    metadata['semantic_tags'][key] = 'default'
    metadata['predictive_score'][key] = 1
    metadata['adaptive_priority'][key] = 1
    metadata['coherence_flag'][key] = True
    metadata['scalability_factor'] = cache_snapshot.size / cache_snapshot.capacity

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Immediately after evicting a victim, the policy removes all metadata associated with the evicted object, recalculates the scalability factor to reflect the current cache size and load conditions, re-evaluates the predictive scores and adaptive priorities of remaining objects, and updates the coherence flags to ensure data integrity.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    evicted_key = evicted_obj.key
    del metadata['access_frequency'][evicted_key]
    del metadata['last_access_time'][evicted_key]
    del metadata['predicted_future_access'][evicted_key]
    del metadata['latency_sensitivity'][evicted_key]
    del metadata['semantic_tags'][evicted_key]
    del metadata['predictive_score'][evicted_key]
    del metadata['adaptive_priority'][evicted_key]
    del metadata['coherence_flag'][evicted_key]
    
    metadata['scalability_factor'] = cache_snapshot.size / cache_snapshot.capacity
    
    for key in cache_snapshot.cache:
        # Assume re-evaluation of predictive scores and adaptive priorities are done here
        metadata['predictive_score'][key] += 1
        metadata['adaptive_priority'][key] += 1
        metadata['coherence_flag'][key] = True
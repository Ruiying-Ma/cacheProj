# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
from collections import deque, defaultdict
import heapq

# Put tunable constant parameters below
SQ_CAPACITY = 10
MQ_CAPACITY = 20
GQ_CAPACITY = 30

# Put the metadata specifically maintained by the policy below. The policy maintains access frequency, recency, predicted future access time, and resource usage statistics for each cached object. It also uses LFU, SQ, MQ, and GQ queues to manage objects based on frequency and recency.
access_frequency = defaultdict(int)
recency = {}
predicted_future_access_time = {}
resource_usage = {}
LFU_queue = []
SQ = deque()
MQ = deque()
GQ = deque()

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy first checks if SQ exceeds its capacity and moves objects to MQ if necessary. If MQ is full, it reduces the frequency of objects cyclically until an object with zero frequency is found and evicts it. If SQ is not full, it evicts the least-frequently-used and least-recently-used object from the LFU queue. It also considers predicted future access and resource usage to prioritize eviction candidates.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Your code below
    if len(SQ) > SQ_CAPACITY:
        while len(SQ) > SQ_CAPACITY:
            moved_obj = SQ.popleft()
            MQ.append(moved_obj)
    
    if len(MQ) > MQ_CAPACITY:
        while len(MQ) > MQ_CAPACITY:
            freq_reduced = False
            for key in list(MQ):
                access_frequency[key] -= 1
                if access_frequency[key] == 0:
                    candid_obj_key = key
                    MQ.remove(key)
                    freq_reduced = True
                    break
            if not freq_reduced:
                candid_obj_key = MQ.popleft()
    
    if candid_obj_key is None:
        if len(SQ) < SQ_CAPACITY:
            if LFU_queue:
                candid_obj_key = heapq.heappop(LFU_queue)[1]
        else:
            candid_obj_key = SQ.popleft()
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy increases the object's frequency by 1, updates its recency to the current timestamp, and recalculates its predicted future access time. It adjusts the object's position in the LFU queue if necessary and updates resource usage statistics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    # Your code below
    access_frequency[obj.key] += 1
    recency[obj.key] = cache_snapshot.access_count
    predicted_future_access_time[obj.key] = cache_snapshot.access_count + 100  # Example prediction logic
    resource_usage[obj.key] = obj.size
    
    # Adjust position in LFU queue
    for i, (freq, key) in enumerate(LFU_queue):
        if key == obj.key:
            LFU_queue[i] = (access_frequency[obj.key], obj.key)
            heapq.heapify(LFU_queue)
            break

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy sets its frequency to 1, recency to the current timestamp, and estimates its predicted future access time. If the object was in GQ, it moves it to the rear of MQ; otherwise, it places it at the rear of SQ and in the appropriate place in the LFU queue. It also initializes resource usage statistics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    # Your code below
    access_frequency[obj.key] = 1
    recency[obj.key] = cache_snapshot.access_count
    predicted_future_access_time[obj.key] = cache_snapshot.access_count + 100  # Example prediction logic
    resource_usage[obj.key] = obj.size
    
    if obj.key in GQ:
        GQ.remove(obj.key)
        MQ.append(obj.key)
    else:
        SQ.append(obj.key)
    
    heapq.heappush(LFU_queue, (access_frequency[obj.key], obj.key))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an object, the policy removes it from the LFU queue and places it at the rear of GQ. It updates overall resource usage statistics and recalibrates the predictive model to improve future eviction decisions. If GQ is full, it removes the front object from GQ.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Your code below
    for i, (freq, key) in enumerate(LFU_queue):
        if key == evicted_obj.key:
            LFU_queue.pop(i)
            heapq.heapify(LFU_queue)
            break
    
    GQ.append(evicted_obj.key)
    if len(GQ) > GQ_CAPACITY:
        GQ.popleft()
    
    # Update resource usage statistics
    del access_frequency[evicted_obj.key]
    del recency[evicted_obj.key]
    del predicted_future_access_time[evicted_obj.key]
    del resource_usage[evicted_obj.key]
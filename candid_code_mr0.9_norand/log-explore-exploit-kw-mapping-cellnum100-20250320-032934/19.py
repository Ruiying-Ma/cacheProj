# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import time

# Put tunable constant parameters below
USAGE_THRESHOLD = 10
BASE_USAGE_COUNTER = 1
BASE_ACCESS_FREQUENCY = 1
BASE_CRITICALITY_SCORE = 1
BASE_CONTEXT_RELEVANCE_SCORE = 1
BASE_DYNAMIC_ADJUSTMENT_FACTOR = 1

# Put the metadata specifically maintained by the policy below. The policy maintains a usage counter, tier level, context relevance score, access frequency, recency timestamp, criticality score, and dynamic adjustment factor for each cached item.
metadata = {}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy first considers items in the lowest tier and selects candidates with the lowest criticality score. Among these, it uses a weighted combination of context relevance score, access frequency, and recency to shortlist candidates. Finally, it randomly selects one of the shortlisted candidates for eviction.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Your code below
    lowest_tier = min(metadata.values(), key=lambda x: x['tier'])['tier']
    candidates = [key for key, data in metadata.items() if data['tier'] == lowest_tier]
    candidates.sort(key=lambda key: metadata[key]['criticality_score'])
    
    if candidates:
        shortlisted = candidates[:1]  # Since we cannot use randomness, we take the first one
        candid_obj_key = shortlisted[0]
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy increments the usage counter and access frequency, updates the recency timestamp, recalculates the context relevance score using the machine learning model, and adjusts the dynamic adjustment factor based on the current workload pattern. It may also promote the item to a higher tier if its usage counter exceeds a threshold.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    metadata[key]['usage_counter'] += 1
    metadata[key]['access_frequency'] += 1
    metadata[key]['recency_timestamp'] = cache_snapshot.access_count
    metadata[key]['context_relevance_score'] = calculate_context_relevance_score(obj)
    metadata[key]['dynamic_adjustment_factor'] = adjust_dynamic_factor(cache_snapshot)
    
    if metadata[key]['usage_counter'] > USAGE_THRESHOLD:
        metadata[key]['tier'] += 1

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes the usage counter and access frequency to base values, sets the recency timestamp, assigns a criticality score based on the object's importance, calculates the initial context relevance score using the machine learning model, assigns it to the lowest tier, and sets the dynamic adjustment factor according to the current workload pattern.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    metadata[key] = {
        'usage_counter': BASE_USAGE_COUNTER,
        'access_frequency': BASE_ACCESS_FREQUENCY,
        'recency_timestamp': cache_snapshot.access_count,
        'criticality_score': calculate_criticality_score(obj),
        'context_relevance_score': calculate_context_relevance_score(obj),
        'tier': 0,
        'dynamic_adjustment_factor': adjust_dynamic_factor(cache_snapshot)
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Following an eviction, the policy resets the usage counters of all remaining items, potentially recalculates their context relevance scores, adjusts their tier levels based on the updated usage counters and relevance scores, recalibrates the dynamic adjustment factor to account for the change in cache load, and updates the overall access pattern statistics.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Your code below
    del metadata[evicted_obj.key]
    for key in metadata:
        metadata[key]['usage_counter'] = BASE_USAGE_COUNTER
        metadata[key]['context_relevance_score'] = calculate_context_relevance_score(cache_snapshot.cache[key])
        if metadata[key]['usage_counter'] > USAGE_THRESHOLD:
            metadata[key]['tier'] += 1
        metadata[key]['dynamic_adjustment_factor'] = adjust_dynamic_factor(cache_snapshot)

def calculate_criticality_score(obj):
    # Placeholder for actual criticality score calculation
    return BASE_CRITICALITY_SCORE

def calculate_context_relevance_score(obj):
    # Placeholder for actual context relevance score calculation
    return BASE_CONTEXT_RELEVANCE_SCORE

def adjust_dynamic_factor(cache_snapshot):
    # Placeholder for actual dynamic adjustment factor calculation
    return BASE_DYNAMIC_ADJUSTMENT_FACTOR
# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import time

# Put tunable constant parameters below
RECENCY_WEIGHT = 0.5
FREQUENCY_WEIGHT = 0.3
SIZE_WEIGHT = 0.1
PREDICTION_WEIGHT = 0.1
PROMOTION_THRESHOLD = 0.7

# Put the metadata specifically maintained by the policy below. Each cached item maintains a timestamp of its last access, an access frequency counter, a priority score calculated as a weighted sum of recency and frequency, a size attribute, and a machine learning-based prediction score. The cache also tracks the cache level of each entry in a multi-level hierarchy and maintains a global schedule to predict future access patterns.
metadata = {}

def calculate_priority_score(recency, frequency):
    return RECENCY_WEIGHT * recency + FREQUENCY_WEIGHT * frequency

def calculate_composite_score(recency, frequency, size, prediction_score, priority_score):
    return (RECENCY_WEIGHT * recency + 
            FREQUENCY_WEIGHT * frequency + 
            SIZE_WEIGHT * size + 
            PREDICTION_WEIGHT * prediction_score + 
            priority_score)

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy calculates a composite score from the multi-dimensional scores (recency, frequency, size, and prediction score) and the priority score. It evicts the item with the lowest composite score, with a preference for evicting items from lower cache levels first. The global schedule is cross-referenced to ensure items predicted to be accessed soon are retained.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    lowest_score = float('inf')
    
    for key, cached_obj in cache_snapshot.cache.items():
        meta = metadata[key]
        recency = cache_snapshot.access_count - meta['last_access']
        frequency = meta['access_frequency']
        size = cached_obj.size
        prediction_score = meta['prediction_score']
        priority_score = meta['priority_score']
        composite_score = calculate_composite_score(recency, frequency, size, prediction_score, priority_score)
        
        if composite_score < lowest_score:
            lowest_score = composite_score
            candid_obj_key = key
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the item's timestamp is updated to the current time, its access frequency counter is incremented, its priority score is recalculated, and its prediction score is updated. The entry may be promoted to a higher cache level if its updated composite score surpasses a threshold.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    meta = metadata[obj.key]
    meta['last_access'] = cache_snapshot.access_count
    meta['access_frequency'] += 1
    meta['priority_score'] = calculate_priority_score(meta['last_access'], meta['access_frequency'])
    # Update prediction score based on some model (not implemented here)
    meta['prediction_score'] = 0.5  # Placeholder value
    
    recency = cache_snapshot.access_count - meta['last_access']
    frequency = meta['access_frequency']
    size = obj.size
    prediction_score = meta['prediction_score']
    priority_score = meta['priority_score']
    composite_score = calculate_composite_score(recency, frequency, size, prediction_score, priority_score)
    
    if composite_score > PROMOTION_THRESHOLD:
        meta['cache_level'] = min(meta['cache_level'] + 1, 3)  # Assuming 3 levels of cache

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, its timestamp is set to the current time, its access frequency counter is initialized to 1, its priority score is calculated, its size is recorded, and its prediction score is initialized. The entry is assigned to the lowest cache level, and the global schedule and prediction model are updated with the new access pattern data.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    metadata[obj.key] = {
        'last_access': cache_snapshot.access_count,
        'access_frequency': 1,
        'priority_score': calculate_priority_score(cache_snapshot.access_count, 1),
        'size': obj.size,
        'prediction_score': 0.5,  # Placeholder value
        'cache_level': 1  # Lowest cache level
    }
    # Update global schedule and prediction model (not implemented here)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    After evicting an item, its metadata is removed, the global schedule is adjusted to remove the evicted item from future predictions, and the prediction model is updated to reflect the change in cache content. The composite scores of remaining entries are recalculated if necessary.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    if evicted_obj.key in metadata:
        del metadata[evicted_obj.key]
    # Adjust global schedule and update prediction model (not implemented here)
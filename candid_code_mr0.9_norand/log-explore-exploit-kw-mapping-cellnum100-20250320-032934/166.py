# Import anything you need below. You must not use any randomness. For example, you cannot `import random`. Also, you cannot use any function in `numpy` that uses randomness, such as the functions in `numpy.random`.
import collections

# Put tunable constant parameters below
USAGE_COUNTER_THRESHOLD = 10
BASE_USAGE_COUNTER = 1
DEFAULT_CONTEXT_RELEVANCE_SCORE = 0.5
DEFAULT_ACCESS_FREQUENCY = 0
DEFAULT_MODIFICATION_FREQUENCY = 0
DEFAULT_PRECISION_SCORE = 0.5
DEFAULT_ACCOUNTABILITY_SCORE = 0.5

# Put the metadata specifically maintained by the policy below. The policy maintains a LRU queue, usage counter, tier level indicator, context relevance score, graph structure of cached items, machine learning model for predicting access patterns and context relevance, access frequency, modification frequency, precision score, and accountability score. It also keeps a global control structure to track overall access and modification patterns.
lru_queue = collections.OrderedDict()
usage_counter = {}
tier_level = {}
context_relevance_score = {}
graph_structure = collections.defaultdict(set)
access_frequency = {}
modification_frequency = {}
precision_score = {}
accountability_score = {}
global_control_structure = {
    'access_patterns': collections.defaultdict(int),
    'modification_patterns': collections.defaultdict(int)
}

def evict(cache_snapshot, obj):
    '''
    This function defines how the policy chooses the eviction victim.
    The policy chooses the eviction victim by first considering items in the lowest tier, selecting the item with the least number of connections in the graph and the lowest combination of context relevance score, access frequency, precision score, and accountability score. If multiple items have the same score, the item at the least-recently-used end of the LRU queue is evicted.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    # Your code below
    lowest_tier = min(tier_level.values())
    candidates = [key for key, tier in tier_level.items() if tier == lowest_tier]
    
    def score(key):
        return (len(graph_structure[key]), 
                context_relevance_score[key], 
                access_frequency[key], 
                precision_score[key], 
                accountability_score[key])
    
    candidates.sort(key=lambda key: (score(key), lru_queue[key]))
    candid_obj_key = candidates[0]
    
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the policy update the metadata it maintains immediately after a cache hit.
    Upon a cache hit, the policy increments the usage counter, access frequency, and accountability score of the accessed item, updates its context relevance score using the machine learning model, reinforces the connections in the graph, may promote the item to a higher tier if its usage counter exceeds a threshold, sets the item's recency as the current timestamp, moves it to the most-recently-used end of the LRU queue, and updates the global control structure to reflect the recent access pattern.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    usage_counter[key] += 1
    access_frequency[key] += 1
    accountability_score[key] += 1
    context_relevance_score[key] = predict_context_relevance(obj)
    reinforce_connections(key)
    
    if usage_counter[key] > USAGE_COUNTER_THRESHOLD:
        tier_level[key] += 1
    
    lru_queue.move_to_end(key)
    global_control_structure['access_patterns'][key] += 1

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after inserting a new object into the cache.
    After inserting a new object, the policy initializes its usage counter to a base value, assigns the item to the lowest tier, calculates its initial context relevance score using the machine learning model, adds a new node to the graph, initializes its connections based on recent access patterns, sets the item's recency as the current timestamp, puts it at the most-recently-used end of the LRU queue, initializes its access frequency, modification frequency, precision score, and accountability score with default values, and updates the global control structure to account for the new item.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    # Your code below
    key = obj.key
    usage_counter[key] = BASE_USAGE_COUNTER
    tier_level[key] = 0
    context_relevance_score[key] = predict_context_relevance(obj)
    graph_structure[key] = set()
    lru_queue[key] = cache_snapshot.access_count
    access_frequency[key] = DEFAULT_ACCESS_FREQUENCY
    modification_frequency[key] = DEFAULT_MODIFICATION_FREQUENCY
    precision_score[key] = DEFAULT_PRECISION_SCORE
    accountability_score[key] = DEFAULT_ACCOUNTABILITY_SCORE
    global_control_structure['access_patterns'][key] = 0
    global_control_structure['modification_patterns'][key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the policy updates the metadata it maintains immediately after evicting the victim.
    Following an eviction, the policy resets the usage counters of all remaining items, recalculates their context relevance scores, adjusts their tier levels, removes the corresponding node and its edges from the graph, updates the machine learning model to exclude the evicted item, removes the evicted item from the LRU queue, removes the metadata of the evicted item, and updates the global control structure to reflect the removal.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    # Your code below
    evicted_key = evicted_obj.key
    del usage_counter[evicted_key]
    del tier_level[evicted_key]
    del context_relevance_score[evicted_key]
    del graph_structure[evicted_key]
    del access_frequency[evicted_key]
    del modification_frequency[evicted_key]
    del precision_score[evicted_key]
    del accountability_score[evicted_key]
    del lru_queue[evicted_key]
    del global_control_structure['access_patterns'][evicted_key]
    del global_control_structure['modification_patterns'][evicted_key]
    
    for key in cache_snapshot.cache:
        usage_counter[key] = BASE_USAGE_COUNTER
        context_relevance_score[key] = predict_context_relevance(cache_snapshot.cache[key])
        if usage_counter[key] > USAGE_COUNTER_THRESHOLD:
            tier_level[key] += 1

def predict_context_relevance(obj):
    # Dummy function to simulate context relevance prediction
    return DEFAULT_CONTEXT_RELEVANCE_SCORE

def reinforce_connections(key):
    # Dummy function to simulate reinforcing connections in the graph
    pass